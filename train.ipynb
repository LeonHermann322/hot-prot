{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "In this notebook we classify the CT/PET scans of lung cancer cases by tumor type. \n",
    "We are doing this as a baseline task to validate our data loading pipeline.\n",
    "Most code is taken from [this tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from thermostability.thermo_dataset import ThermostabilityDataset\n",
    "from util.telegram import TelegramBot\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.list_gpu_processes()\n",
    "\n",
    "telegramBot = TelegramBot()\n",
    "telegramBot.enabled=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining datasets (train/validation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 12, 'val': 12}\n"
     ]
    }
   ],
   "source": [
    "trainSet = ThermostabilityDataset(\"train.csv\", limit=12)\n",
    "valSet = ThermostabilityDataset(\"train.csv\", limit=12)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(trainSet, batch_size=2, shuffle=True, num_workers=4),\n",
    "    \"val\": torch.utils.data.DataLoader(valSet, batch_size=2, shuffle=True, num_workers=4)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\"train\": len(trainSet),\"val\": len(valSet)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [('MKDVVIQTVSANRHNYTKLAMVQFDTAAGLENITYFDDVLSFSSALNSNPPNSKNAASSTQTSDVLSVVSGFLAASGRPLESSMVVLLVNRWPAANADLTSSEYDTITNNNVKIFPISSANSYVQQIPIVTASAKIFTTLAANSNAHFVLGDYGKPLAQVFQYLMGTAYLDSLTITRSFADRTNAAQKQIGKLRVPHSETQSYVNFTITISVGLNGWGNFNYPNTNGIEVTFYQSQANQKTVQFEPGSLQGTNFYYATVQLKESSTYQVIYQSSLIDGSVAMVRVWTTSALYHYGSYATLEDPMGGNTLDKVDEYQGAALRMKLMNDCYTSHAGYAVFTDCTGAVSSKYDASQTIPIDYIFADEGSFPHYPIVPFFCDSKPKSTVDCVPGTESKYDIQFVAGEFIVNRSFQCRPGVGQINPNCTNVDSNGNYYCNRDQLPYMRGPTGQIPDCLGHGHVEYDFAFAEAYICVCDNSYSGDSCQIKN', 'MEHKGKNDFHSEWAKSIKELMLSLHEYVRQHHTTGLVWNSDPGATPMCNRKSGGAPTPPPPPPPPISLIAPSKPSGVGALLESLNTGLSATSRLKKVTPEMQTHKNPVLREVNGQMNRKTEERKVSENKKPEKIHESSIFWDGKIWKVDHQVGNKNAVVEVTDKKESIYIYKCNDSIIKIKGKANAITLDGCRKTSVVFDGLVAQCEIINCQSIQIQTLGELPTVSIQKTDGCHIYLSRDALNAQIVASKSSEMNISAMLEDGDDEYTEMALPEQFMTKIVGKKLVTVASEIV'), tensor([50.7222, 42.6682])])\n"
     ]
    }
   ],
   "source": [
    "print(next(enumerate(dataloaders[\"train\"])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "from thermostability.hotinfer import HotInferModelParallel\n",
    "model = HotInferModelParallel()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from util.train import train_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 1353.4812: [6/6], loss: 447.478790, batch abs diff mean 20.8033072\n",
      "val Loss: 271.2357tch: [6/6], loss: 59.619999, batch abs diff mean 7.42116592\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 188.2080h: [6/6], loss: 491.958099, batch abs diff mean 20.411989\n",
      "val Loss: 675.0450tch: [6/6], loss: 365.298279, batch abs diff mean 19.1127747\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 286.5279h: [6/6], loss: 66.805275, batch abs diff mean 6.69361957\n",
      "val Loss: 115.9367tch: [6/6], loss: 168.285034, batch abs diff mean 11.106337\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 398.4610h: [6/6], loss: 634.274109, batch abs diff mean 25.133234\n",
      "val Loss: 86.4151atch: [6/6], loss: 2.281574, batch abs diff mean 1.277910570\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 269.3796h: [6/6], loss: 39.430687, batch abs diff mean 5.38305780\n",
      "val Loss: 359.6145tch: [6/6], loss: 337.356323, batch abs diff mean 16.737181\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 493.1298h: [6/6], loss: 161.294739, batch abs diff mean 11.0451222\n",
      "val Loss: 152.6251tch: [6/6], loss: 64.335243, batch abs diff mean 7.54471676\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 144.8491h: [6/6], loss: 71.228127, batch abs diff mean 6.68643016\n",
      "val Loss: 43.0934atch: [6/6], loss: 17.147728, batch abs diff mean 4.009594\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 58.5575ch: [6/6], loss: 72.068848, batch abs diff mean 8.47253656\n",
      "val Loss: 58.3184atch: [6/6], loss: 9.426075, batch abs diff mean 2.487553861\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 48.2430ch: [6/6], loss: 72.823807, batch abs diff mean 6.670200\n",
      "val Loss: 43.5419atch: [6/6], loss: 56.500687, batch abs diff mean 7.467739\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 50.0675ch: [6/6], loss: 90.620506, batch abs diff mean 8.912046\n",
      "val Loss: 31.9625atch: [6/6], loss: 3.187545, batch abs diff mean 1.5617647\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 45.2120tch: [6/6], loss: 60.859768, batch abs diff mean 7.206623\n",
      "val Loss: 40.0848Batch: [6/6], loss: 41.648430, batch abs diff mean 5.052446\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 43.8748tch: [6/6], loss: 24.895332, batch abs diff mean 3.945408\n",
      "val Loss: 39.7111Batch: [6/6], loss: 31.656218, batch abs diff mean 5.471960\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 38.1554tch: [6/6], loss: 79.636940, batch abs diff mean 8.836412\n",
      "val Loss: 54.5594Batch: [6/6], loss: 0.800500, batch abs diff mean 0.73773867\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 57.5706tch: [6/6], loss: 4.060408, batch abs diff mean 1.52313240\n",
      "val Loss: 42.3158Batch: [6/6], loss: 100.009789, batch abs diff mean 9.865902\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 49.9140tch: [6/6], loss: 60.483807, batch abs diff mean 6.77641575\n",
      "val Loss: 40.3751Batch: [6/6], loss: 28.706791, batch abs diff mean 3.82769614\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 44.4861tch: [6/6], loss: 13.358284, batch abs diff mean 3.531218\n",
      "val Loss: 43.3708Batch: [6/6], loss: 72.458115, batch abs diff mean 8.132488\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 50.4282tch: [6/6], loss: 36.342632, batch abs diff mean 5.2607612\n",
      "val Loss: 46.8101Batch: [6/6], loss: 102.919319, batch abs diff mean 8.199455\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 49.9474tch: [6/6], loss: 2.657529, batch abs diff mean 1.32692016\n",
      "val Loss: 38.4594Batch: [6/6], loss: 45.591015, batch abs diff mean 6.556711\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 44.0383tch: [6/6], loss: 24.327120, batch abs diff mean 4.82299454\n",
      "val Loss: 45.8319Batch: [6/6], loss: 108.573227, batch abs diff mean 9.690308\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 39.0691tch: [6/6], loss: 75.780067, batch abs diff mean 7.306633\n",
      "val Loss: 47.3863Batch: [6/6], loss: 51.918304, batch abs diff mean 7.190027\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 45.3387tch: [6/6], loss: 15.419268, batch abs diff mean 3.65536723\n",
      "val Loss: 42.0993Batch: [6/6], loss: 97.307968, batch abs diff mean 9.863173\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 54.4650tch: [6/6], loss: 16.096678, batch abs diff mean 3.892576\n",
      "val Loss: 50.6983Batch: [6/6], loss: 75.467476, batch abs diff mean 8.4729712\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 51.5418tch: [6/6], loss: 128.470856, batch abs diff mean 10.731871\n",
      "val Loss: 51.3428Batch: [6/6], loss: 36.007244, batch abs diff mean 4.72109290\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 50.5676tch: [6/6], loss: 126.245857, batch abs diff mean 10.349562\n",
      "val Loss: 49.7696Batch: [6/6], loss: 7.882399, batch abs diff mean 2.13530588\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 54.1623tch: [6/6], loss: 0.473362, batch abs diff mean 0.509113833\n",
      "val Loss: 57.5959Batch: [6/6], loss: 8.000815, batch abs diff mean 2.05454124\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 58.7598tch: [6/6], loss: 126.109108, batch abs diff mean 9.728579\n",
      "val Loss: 58.0172Batch: [6/6], loss: 114.040741, batch abs diff mean 9.0427218\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 60.9023tch: [6/6], loss: 52.535751, batch abs diff mean 6.6257040\n",
      "val Loss: 62.0395Batch: [6/6], loss: 1.473656, batch abs diff mean 1.198172247\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 62.6570tch: [6/6], loss: 169.892349, batch abs diff mean 12.639893\n",
      "val Loss: 64.5035Batch: [6/6], loss: 49.454956, batch abs diff mean 6.19813375\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 67.3338tch: [6/6], loss: 14.168736, batch abs diff mean 3.3720618\n",
      "val Loss: 68.7793Batch: [6/6], loss: 130.121964, batch abs diff mean 9.0250828\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 70.3058tch: [6/6], loss: 95.189491, batch abs diff mean 9.00666802\n",
      "val Loss: 73.0408Batch: [6/6], loss: 181.619110, batch abs diff mean 13.460932\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 75.1599tch: [6/6], loss: 59.907078, batch abs diff mean 6.37893965\n",
      "val Loss: 77.2397Batch: [6/6], loss: 61.710388, batch abs diff mean 6.73970050\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 79.0996tch: [6/6], loss: 112.254280, batch abs diff mean 8.3107017\n",
      "val Loss: 81.7681Batch: [6/6], loss: 63.704845, batch abs diff mean 7.77842319\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 84.3914tch: [6/6], loss: 209.583969, batch abs diff mean 14.453630\n",
      "val Loss: 86.6178Batch: [6/6], loss: 98.623459, batch abs diff mean 7.69524650\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 88.1153tch: [6/6], loss: 6.464911, batch abs diff mean 2.143175918\n",
      "val Loss: 90.3453Batch: [6/6], loss: 208.632645, batch abs diff mean 13.975891\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 92.4212tch: [6/6], loss: 197.799438, batch abs diff mean 13.906981\n",
      "val Loss: 96.3997Batch: [6/6], loss: 178.058548, batch abs diff mean 13.119236\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 97.1101tch: [6/6], loss: 101.648697, batch abs diff mean 9.8286215\n",
      "val Loss: 101.2395atch: [6/6], loss: 115.030365, batch abs diff mean 9.523584\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 102.8142ch: [6/6], loss: 69.006844, batch abs diff mean 7.80242357\n",
      "val Loss: 106.1374atch: [6/6], loss: 1.583312, batch abs diff mean 1.047003321\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 107.0820ch: [6/6], loss: 298.744843, batch abs diff mean 17.140656\n",
      "val Loss: 110.5694atch: [6/6], loss: 13.492309, batch abs diff mean 2.64692910\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 110.0675ch: [6/6], loss: 2.769523, batch abs diff mean 1.226986979\n",
      "val Loss: 114.0303atch: [6/6], loss: 16.854830, batch abs diff mean 3.57149998\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 119.8748ch: [6/6], loss: 52.166939, batch abs diff mean 6.95549690\n",
      "val Loss: 121.2913atch: [6/6], loss: 110.816948, batch abs diff mean 9.5027906\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 120.5304ch: [6/6], loss: 348.273010, batch abs diff mean 18.650402\n",
      "val Loss: 121.9419atch: [6/6], loss: 116.274323, batch abs diff mean 9.7683071\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 125.7434ch: [6/6], loss: 291.306702, batch abs diff mean 17.067038\n",
      "val Loss: 127.6675atch: [6/6], loss: 56.523678, batch abs diff mean 7.44774664\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 129.3513ch: [6/6], loss: 39.760727, batch abs diff mean 5.39833545\n",
      "val Loss: 134.2354atch: [6/6], loss: 17.872206, batch abs diff mean 3.87548335\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 135.1465ch: [6/6], loss: 100.089874, batch abs diff mean 8.1635571\n",
      "val Loss: 134.6028atch: [6/6], loss: 42.659210, batch abs diff mean 5.68230655\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 138.8825ch: [6/6], loss: 30.462315, batch abs diff mean 5.05249009\n",
      "val Loss: 142.6632atch: [6/6], loss: 230.835266, batch abs diff mean 13.760214\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 145.2617ch: [6/6], loss: 397.986084, batch abs diff mean 19.866974\n",
      "val Loss: 149.7463atch: [6/6], loss: 328.074707, batch abs diff mean 18.062996\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 150.0736ch: [6/6], loss: 196.820938, batch abs diff mean 13.016081\n",
      "val Loss: 155.4789atch: [6/6], loss: 114.423866, batch abs diff mean 10.400331\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 157.3408ch: [6/6], loss: 82.635551, batch abs diff mean 8.80272366\n",
      "val Loss: 154.8274atch: [6/6], loss: 176.642517, batch abs diff mean 10.969931\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 154.2027ch: [6/6], loss: 124.198914, batch abs diff mean 9.9963006\n",
      "val Loss: 159.5092atch: [6/6], loss: 220.575394, batch abs diff mean 14.851437\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 162.0088ch: [6/6], loss: 43.167492, batch abs diff mean 6.13479410\n",
      "val Loss: 163.8107atch: [6/6], loss: 270.830719, batch abs diff mean 16.222425\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 168.2373ch: [6/6], loss: 34.019764, batch abs diff mean 5.63152522\n",
      "val Loss: 177.8830atch: [6/6], loss: 41.474880, batch abs diff mean 5.64616078\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 171.7398ch: [6/6], loss: 172.768021, batch abs diff mean 12.225029\n",
      "val Loss: 178.2804atch: [6/6], loss: 293.376343, batch abs diff mean 15.189636\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 171.0466ch: [6/6], loss: 139.654877, batch abs diff mean 10.783932\n",
      "val Loss: 181.4262atch: [6/6], loss: 140.287903, batch abs diff mean 10.815292\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 183.0109ch: [6/6], loss: 348.692749, batch abs diff mean 18.477520\n",
      "val Loss: 176.6489atch: [6/6], loss: 176.038803, batch abs diff mean 11.462435\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 187.5460ch: [6/6], loss: 311.555084, batch abs diff mean 15.784950\n",
      "val Loss: 190.0955atch: [6/6], loss: 50.999836, batch abs diff mean 6.40422174\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 188.4280ch: [6/6], loss: 26.924931, batch abs diff mean 4.93627007\n",
      "val Loss: 191.8516atch: [6/6], loss: 165.618515, batch abs diff mean 10.624775\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 196.3547ch: [6/6], loss: 55.940125, batch abs diff mean 6.73795921\n",
      "val Loss: 202.4417atch: [6/6], loss: 269.593231, batch abs diff mean 14.921984\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 199.9456ch: [6/6], loss: 93.114677, batch abs diff mean 8.40972597\n",
      "val Loss: 193.0072atch: [6/6], loss: 253.780411, batch abs diff mean 15.123901\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 206.3896ch: [6/6], loss: 351.759613, batch abs diff mean 18.638420\n",
      "val Loss: 205.5243atch: [6/6], loss: 52.514061, batch abs diff mean 7.10803872\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 208.7218ch: [6/6], loss: 263.141632, batch abs diff mean 15.449049\n",
      "val Loss: 209.0269atch: [6/6], loss: 204.338928, batch abs diff mean 14.274300\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 210.7006ch: [6/6], loss: 96.505219, batch abs diff mean 9.58003960\n",
      "val Loss: 216.0263atch: [6/6], loss: 131.578537, batch abs diff mean 11.200582\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 211.7733ch: [6/6], loss: 66.191200, batch abs diff mean 7.46971094\n",
      "val Loss: 222.0129atch: [6/6], loss: 28.121239, batch abs diff mean 5.21739843\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 222.0676ch: [6/6], loss: 370.460022, batch abs diff mean 19.246357\n",
      "val Loss: 230.0537atch: [6/6], loss: 138.503647, batch abs diff mean 11.499604\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 218.4199ch: [6/6], loss: 454.202026, batch abs diff mean 21.246088\n",
      "val Loss: 228.9472atch: [6/6], loss: 371.175842, batch abs diff mean 18.719986\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 225.9242ch: [6/6], loss: 324.062256, batch abs diff mean 17.680174\n",
      "val Loss: 231.8887atch: [6/6], loss: 540.698120, batch abs diff mean 23.252430\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 230.6388ch: [6/6], loss: 299.184540, batch abs diff mean 15.134447\n",
      "val Loss: 237.9591atch: [6/6], loss: 235.785110, batch abs diff mean 14.502611\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 229.2697ch: [6/6], loss: 43.753334, batch abs diff mean 6.60723939\n",
      "val Loss: 234.6003atch: [6/6], loss: 470.519470, batch abs diff mean 21.688610\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 235.1399ch: [6/6], loss: 226.387146, batch abs diff mean 13.568013\n",
      "val Loss: 245.7215atch: [6/6], loss: 492.179565, batch abs diff mean 21.810974\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 246.8170ch: [6/6], loss: 261.872742, batch abs diff mean 13.717035\n",
      "val Loss: 241.9700atch: [6/6], loss: 247.911240, batch abs diff mean 14.902389\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 242.3021ch: [6/6], loss: 76.412491, batch abs diff mean 8.64897091\n",
      "val Loss: 247.1002atch: [6/6], loss: 171.845337, batch abs diff mean 12.560538\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 247.9119ch: [6/6], loss: 122.135788, batch abs diff mean 10.995095\n",
      "val Loss: 252.6941atch: [6/6], loss: 443.037354, batch abs diff mean 21.034266\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 252.9939ch: [6/6], loss: 189.568832, batch abs diff mean 12.067302\n",
      "val Loss: 253.3296atch: [6/6], loss: 44.225739, batch abs diff mean 6.56358322\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 261.5720ch: [6/6], loss: 171.378677, batch abs diff mean 12.834540\n",
      "val Loss: 255.8433atch: [6/6], loss: 361.791321, batch abs diff mean 18.732674\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 258.4609ch: [6/6], loss: 394.704254, batch abs diff mean 19.846409\n",
      "val Loss: 265.3489atch: [6/6], loss: 85.217468, batch abs diff mean 9.14862957\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 266.7738ch: [6/6], loss: 351.991913, batch abs diff mean 17.381950\n",
      "val Loss: 257.1968atch: [6/6], loss: 133.168335, batch abs diff mean 11.483003\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 259.7490ch: [6/6], loss: 87.986870, batch abs diff mean 9.30345886\n",
      "val Loss: 263.0896atch: [6/6], loss: 330.166321, batch abs diff mean 17.515656\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 272.8327ch: [6/6], loss: 102.894409, batch abs diff mean 10.132701\n",
      "val Loss: 266.2621atch: [6/6], loss: 280.143066, batch abs diff mean 16.497482\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 269.4055ch: [6/6], loss: 141.248383, batch abs diff mean 11.837611\n",
      "val Loss: 270.9014atch: [6/6], loss: 545.880554, batch abs diff mean 22.993782\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 274.5108ch: [6/6], loss: 104.969269, batch abs diff mean 9.6506158\n",
      "val Loss: 269.6170atch: [6/6], loss: 145.419312, batch abs diff mean 11.895888\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 285.4107ch: [6/6], loss: 541.071777, batch abs diff mean 23.186060\n",
      "val Loss: 280.5578atch: [6/6], loss: 257.638245, batch abs diff mean 14.536831\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 278.0496ch: [6/6], loss: 346.727844, batch abs diff mean 16.479126\n",
      "val Loss: 289.5453atch: [6/6], loss: 415.505646, batch abs diff mean 19.614456\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 285.4477ch: [6/6], loss: 293.763824, batch abs diff mean 16.322361\n",
      "val Loss: 290.9457atch: [6/6], loss: 271.852325, batch abs diff mean 15.209659\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 287.9535ch: [6/6], loss: 61.336029, batch abs diff mean 7.74635570\n",
      "val Loss: 294.2949atch: [6/6], loss: 403.052612, batch abs diff mean 19.816578\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 299.7503ch: [6/6], loss: 375.434387, batch abs diff mean 19.209356\n",
      "val Loss: 294.0878atch: [6/6], loss: 353.508423, batch abs diff mean 17.663197\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 296.3987ch: [6/6], loss: 547.360779, batch abs diff mean 22.849323\n",
      "val Loss: 306.5261atch: [6/6], loss: 412.019135, batch abs diff mean 17.341652\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 296.2743ch: [6/6], loss: 424.512726, batch abs diff mean 19.755196\n",
      "val Loss: 306.8764atch: [6/6], loss: 166.245331, batch abs diff mean 11.872677\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 295.1691ch: [6/6], loss: 386.632080, batch abs diff mean 19.502110\n",
      "val Loss: 303.5000atch: [6/6], loss: 67.787552, batch abs diff mean 8.14812943\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 304.4336ch: [6/6], loss: 325.168915, batch abs diff mean 15.774502\n",
      "val Loss: 310.0412atch: [6/6], loss: 113.470413, batch abs diff mean 10.592286\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 314.4196ch: [6/6], loss: 469.392517, batch abs diff mean 20.035910\n",
      "val Loss: 307.4695atch: [6/6], loss: 468.142578, batch abs diff mean 21.396667\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 313.8693ch: [6/6], loss: 250.770020, batch abs diff mean 14.642456\n",
      "val Loss: 316.4900atch: [6/6], loss: 318.975311, batch abs diff mean 17.051674\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 323.3678ch: [6/6], loss: 474.399109, batch abs diff mean 20.129070\n",
      "val Loss: 320.2343atch: [6/6], loss: 368.476227, batch abs diff mean 16.725338\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 319.8739ch: [6/6], loss: 176.154907, batch abs diff mean 13.213626\n",
      "val Loss: 316.4875atch: [6/6], loss: 331.596619, batch abs diff mean 16.809429\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 312.4906ch: [6/6], loss: 227.193573, batch abs diff mean 14.827617\n",
      "val Loss: 320.5182atch: [6/6], loss: 91.529099, batch abs diff mean 9.36814231\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 321.2304ch: [6/6], loss: 260.996857, batch abs diff mean 14.980556\n",
      "val Loss: 333.4477atch: [6/6], loss: 336.674438, batch abs diff mean 18.113716\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 329.6758ch: [6/6], loss: 182.011703, batch abs diff mean 13.428016\n",
      "val Loss: 324.5269atch: [6/6], loss: 413.318176, batch abs diff mean 18.920353\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 324.1934ch: [6/6], loss: 337.019287, batch abs diff mean 17.575962\n",
      "val Loss: 333.4177atch: [6/6], loss: 139.644012, batch abs diff mean 11.264662\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 333.1488ch: [6/6], loss: 350.056976, batch abs diff mean 16.526520\n",
      "val Loss: 328.9885atch: [6/6], loss: 615.179688, batch abs diff mean 24.739658\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 328.8432ch: [6/6], loss: 406.498901, batch abs diff mean 19.622383\n",
      "val Loss: 335.7628atch: [6/6], loss: 619.188782, batch abs diff mean 24.821180\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 331.8881ch: [6/6], loss: 324.098267, batch abs diff mean 17.776081\n",
      "val Loss: 340.2356atch: [6/6], loss: 409.165894, batch abs diff mean 18.521334\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 340.1469ch: [6/6], loss: 196.015564, batch abs diff mean 13.037823\n",
      "val Loss: 342.4530atch: [6/6], loss: 577.057373, batch abs diff mean 23.819548\n",
      "\n",
      "Training complete in 9m 59s\n",
      "Best val Acc: 31.962535\n",
      "Saved predictions as scatter plot at results/predictions_epochs100_gradClip10_trainSize12_valSize12.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    model, best_epoch_loss = train_model(model, criterion, exp_lr_scheduler, dataloaders=dataloaders, dataset_sizes=dataset_sizes, use_wandb=False,\n",
    "                        num_epochs=100, prepare_labels = lambda x: x.to(\"cuda:1\"))\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    if not telegramBot.enabled:\n",
    "        raise e\n",
    "    telegramBot.send_telegram(f\"Training failed with error message: {str(e)}\")                         "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infering thermostability for sample 0/6...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1194564/602196970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Diff: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_diffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdiffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictDiffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#diffs = np.array([0, 0.1, 0.2,-0.2, -0.8, 0.1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1194564/602196970.py\u001b[0m in \u001b[0;36mpredictDiffs\u001b[0;34m(set)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Infering thermostability for sample {index}/{n}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0m_diffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/hot-prot/thermostability/hotinfer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# TODO: optimize (run esmfold for samples while thermomodule runs, replace esmfold by esm2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mesm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesmfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0ms_s_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s_s_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/esmfold.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, sequences, residx, masking_pattern, num_recycles, residue_index_offset, chain_linker)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mresidx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mmasking_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasking_pattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mnum_recycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_recycles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/esmfold.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, aa, mask, residx, masking_pattern, num_recycles)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         structure: dict = self.trunk(\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0ms_s_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_recycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_recycles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         )\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Documenting what we expect:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/trunk.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles)\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0;34m{\u001b[0m\u001b[0;34m\"single\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrunk2sm_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pair\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrunk2sm_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                     \u001b[0mtrue_aa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 )\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/model/structure_module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, evoformer_output_dict, aatype, mask, inplace_safe, _offload_inference)\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0minplace_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0m_offload_inference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_offload_inference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                 \u001b[0m_z_reference_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_reference_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m             )\n\u001b[1;32m    682\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipa_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/model/structure_module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s, z, r, mask, inplace_safe, _offload_inference, _z_reference_list)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mkv_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_pts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mkv_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mkv_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv_pts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# [*, N_res, H, (P_q + P_v), 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/utils/rigid_utils.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, pts)\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \"\"\"\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0mrotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrotated\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/utils/rigid_utils.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, pts)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \"\"\"\n\u001b[1;32m    626\u001b[0m         \u001b[0mrot_mats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rot_mats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrot_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrot_mats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvert_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/utils/rigid_utils.py\u001b[0m in \u001b[0;36mrot_vec_mul\u001b[0;34m(r, t)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         ],\n\u001b[1;32m     85\u001b[0m         \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def predictDiffs(set=\"val\"):\n",
    "    with torch.no_grad():\n",
    "        n = len(dataloaders[set])\n",
    "        diffs = torch.tensor([])\n",
    "        for index, (inputs, labels) in enumerate(dataloaders[set]):\n",
    "            #inputs = inputs.to(device)\n",
    "            print(f\"Infering thermostability for sample {index}/{n}...\")\n",
    "            labels = labels.to(\"cuda:1\")\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _diffs = outputs.squeeze().sub(labels.squeeze()).cpu()\n",
    "            diffs = torch.cat((diffs, _diffs))\n",
    "            print(\"Diff: \", _diffs)\n",
    "    return diffs\n",
    "diffs = predictDiffs()\n",
    "\n",
    "#diffs = np.array([0, 0.1, 0.2,-0.2, -0.8, 0.1])\n",
    "plt.title(\"Differences predicted <-> actual thermostability\")\n",
    "plt.hist(diffs, 10)\n",
    "resultsDir = \"results\"\n",
    "now = datetime.now()\n",
    "time = now.strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "os.makedirs(resultsDir, exist_ok=True)\n",
    "histFile = f\"results/{time}_diffs.png\"\n",
    "plt.savefig(histFile)\n",
    "telegramBot.send_photo(histFile, f\"Differences predicted <-> actual thermostability at {time}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    modelPath = os.path.join(resultsDir, f\"{time}_model.pth\")\n",
    "    torch.save(model, modelPath)\n",
    "    telegramBot.send_telegram(f\"Model saved at {modelPath}\")\n",
    "except Exception as e:\n",
    "    telegramBot.send_telegram(f\"Saving model failed for reason: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('hotprot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ce9c753ae11230af47747c2525775742a5c4219355d1b58136056d9f8dd6ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
