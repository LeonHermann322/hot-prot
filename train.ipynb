{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "In this notebook we classify the CT/PET scans of lung cancer cases by tumor type. \n",
    "We are doing this as a baseline task to validate our data loading pipeline.\n",
    "Most code is taken from [this tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pynvml module not found, please install pynvml'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from thermostability.thermo_dataset import ThermostabilityDataset\n",
    "\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.list_gpu_processes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining datasets (train/validation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache file:  data/train_sequences.fasta_cache.p\n",
      "Loading data from cache file:  data/eval_sequences.fasta_cache.p\n"
     ]
    }
   ],
   "source": [
    "trainSet = ThermostabilityDataset(\"data/train_sequences.fasta\")\n",
    "valSet = ThermostabilityDataset(\"data/eval_sequences.fasta\")\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(trainSet, batch_size=1, shuffle=True, num_workers=4),\n",
    "    \"val\": torch.utils.data.DataLoader(valSet, batch_size=1, shuffle=True, num_workers=4)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\"train\": len(trainSet),\"val\": len(valSet)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [('MRICFLLLAFLVAETFANELTRCCAGGTRHFKNSNTCSSIKSEGTSMTCQRAASICCLRSLLDNACDSGTDIAKEEESCPSNINILGGGLKKECCDCCLLAKDLLNRNEPCVAPVGFSAGCLRSFNKCCNGDIEITHASEIITGRPLNDPHVLHLGDRCASSHCEHLCHDRGGEKVECSCRSGFDLAPDGMACVDIDECATLMDDCLESQRCLNTPGSFKCIRTLSCGTGYAMDSETERCRDVDECNLGSHDCGPLYQCRNTQGSYRCDAKKCGDGELQNPMTGECTSITCPNGYYPKNGMCNDIDECVTGHNCGAGEECVNTPGSFRCQQKGNLCAHGYEVNGATGFCEDVNECTTGIAACEQKCVNIPGSYQCICDRGFALGPDGTKCEDIDECSIWAGSGNDLCMGGCINTKGSYLCQCPPGYKIQPDGRTCVDVDECAMGECAGSDKVCVNTLGSFKCHSIDCPTNYIHDSLNKNRCNRQPSACGLPEECSKVPLFLTYQFISLARAVPISSHRPAITLFKVSAPNHADTEVNFELQLKTTIVGAPNVLPAIRANFLLQKGEKRNSAVVTLRDSLDGPQTVKLQLLLRMSKKGKNFNTYAANLIVDVAAHKRHNTVHPPLMKIR',), tensor([[43.8922]])])\n"
     ]
    }
   ],
   "source": [
    "print(next(enumerate(dataloaders[\"train\"])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HotInfer(\n",
       "  (esmfold): ESMFold(\n",
       "    (esm): ESM2(\n",
       "      (embed_tokens): Embedding(33, 2560, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (24): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (25): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (26): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (27): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (28): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (29): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (30): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (31): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (32): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (33): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (34): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (35): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (contact_head): ContactPredictionHead(\n",
       "        (regression): Linear(in_features=1440, out_features=1, bias=True)\n",
       "        (activation): Sigmoid()\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        (layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (esm_s_mlp): Sequential(\n",
       "      (0): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=2560, out_features=1024, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(23, 1024, padding_idx=0)\n",
       "    (trunk): FoldingTrunk(\n",
       "      (pairwise_positional_embedding): RelativePosition(\n",
       "        (embedding): Embedding(66, 128)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (24): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (25): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (26): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (27): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (28): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (29): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (30): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (31): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (32): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (33): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (34): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (35): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (36): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (37): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (38): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (39): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (40): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (41): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (42): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (43): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (44): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (45): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (46): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (47): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (recycle_s_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (recycle_z_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (recycle_disto): Embedding(15, 128)\n",
       "      (structure_module): StructureModule(\n",
       "        (layer_norm_s): LayerNorm()\n",
       "        (layer_norm_z): LayerNorm()\n",
       "        (linear_in): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (ipa): InvariantPointAttention(\n",
       "          (linear_q): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (linear_kv): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (linear_q_points): Linear(in_features=384, out_features=144, bias=True)\n",
       "          (linear_kv_points): Linear(in_features=384, out_features=432, bias=True)\n",
       "          (linear_b): Linear(in_features=128, out_features=12, bias=True)\n",
       "          (linear_out): Linear(in_features=2112, out_features=384, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (softplus): Softplus(beta=1, threshold=20)\n",
       "        )\n",
       "        (ipa_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm_ipa): LayerNorm()\n",
       "        (transition): StructureModuleTransition(\n",
       "          (layers): ModuleList(\n",
       "            (0): StructureModuleTransitionLayer(\n",
       "              (linear_1): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (linear_2): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (linear_3): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (relu): ReLU()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "        (bb_update): BackboneUpdate(\n",
       "          (linear): Linear(in_features=384, out_features=6, bias=True)\n",
       "        )\n",
       "        (angle_resnet): AngleResnet(\n",
       "          (linear_in): Linear(in_features=384, out_features=128, bias=True)\n",
       "          (linear_initial): Linear(in_features=384, out_features=128, bias=True)\n",
       "          (layers): ModuleList(\n",
       "            (0): AngleResnetBlock(\n",
       "              (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (relu): ReLU()\n",
       "            )\n",
       "            (1): AngleResnetBlock(\n",
       "              (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (relu): ReLU()\n",
       "            )\n",
       "          )\n",
       "          (linear_out): Linear(in_features=128, out_features=14, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (trunk2sm_s): Linear(in_features=1024, out_features=384, bias=True)\n",
       "      (trunk2sm_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (distogram_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (ptm_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (lm_head): Linear(in_features=1024, out_features=23, bias=True)\n",
       "    (lddt_head): Sequential(\n",
       "      (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=1850, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (thermo_module_rnn): RNN(1024, 128, num_layers=2, batch_first=True)\n",
       "  (thermo_module_regression): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thermostability.hotinfer import HotInfer\n",
    "model = HotInfer()\n",
    "model.esmfold.requires_grad_(False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "def train_model(model, criterion,optimizer , scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_epoch_loss = sys.float_info.max\n",
    "    losses = []\n",
    "    batchEnumeration = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "         \n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                #inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    print(\"Outputs\", outputs)\n",
    "                    print(\"Labels\", labels)\n",
    "                    loss = criterion(outputs,labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                batch_size = len(inputs)\n",
    "                batch_loss = loss.item() * batch_size\n",
    "                losses.append(batch_loss)\n",
    "                batchEnumeration.append(batchEnumeration[-1]+1 if len(batchEnumeration)>0 else 0)\n",
    "\n",
    "                running_loss += batch_loss\n",
    "               \n",
    "            \n",
    "                if idx % 10 == 0:\n",
    "                    batch_size = len(inputs)\n",
    "               \n",
    "                    tqdm.write(\"Epoch: [{}/{}], Batch: [{}/{}], loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        num_epochs,\n",
    "                        idx + 1,\n",
    "                        len(dataloaders[phase]),\n",
    "                        batch_loss / float(batch_size)\n",
    "                        ), end=\"\\r\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_epoch_loss:\n",
    "                best_epoch_loss = epoch_loss\n",
    "                #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_epoch_loss:4f}')\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "Outputs tensor([[0.2047]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Labels tensor([[49.0448]], device='cuda:0')\n",
      "Outputs tensor([[0.4303]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Labels tensor([[41.9732]], device='cuda:0')\n",
      "Outputs tensor([[0.9685]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Labels tensor([[45.6447]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 21.05 GiB (GPU 0; 44.43 GiB total capacity; 34.71 GiB already allocated; 5.77 GiB free; 37.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_383924/3570588032.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                        num_epochs=1)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_383924/1882724256.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/hot-prot/thermostability/hotinfer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     42\u001b[0m         ):\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mesm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesmfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0ms_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s_s\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/esmfold.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, sequences, residx, masking_pattern, num_recycles, residue_index_offset, chain_linker)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mresidx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mmasking_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasking_pattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mnum_recycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_recycles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/esmfold.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, aa, mask, residx, masking_pattern, num_recycles)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         structure: dict = self.trunk(\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0ms_s_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_recycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_recycles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         )\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Documenting what we expect:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/trunk.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles)\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mrecycle_z\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecycle_disto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecycle_bins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0ms_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrunk_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_s_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecycle_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecycle_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;31m# === Structure module ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/trunk.py\u001b[0m in \u001b[0;36mtrunk_iter\u001b[0;34m(s, z, residx, mask)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidue_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/tri_self_attn_block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequence_state, pairwise_state, mask, chunk_size, **_TriangularSelfAttentionBlock__kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         )\n\u001b[1;32m    150\u001b[0m         pairwise_state = pairwise_state + self.row_drop(\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtri_att_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairwise_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtri_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         )\n\u001b[1;32m    153\u001b[0m         pairwise_state = pairwise_state + self.col_drop(\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/model/triangular_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, chunk_size, use_memory_efficient_kernel, use_lma, inplace_safe)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mbiases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0muse_memory_efficient_kernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_memory_efficient_kernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0muse_lma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_lma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             )\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/model/primitives.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q_x, kv_x, biases, use_memory_efficient_kernel, use_lma, lma_q_chunk_size, lma_kv_chunk_size, use_flash, flash_mask)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flash_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflash_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/model/primitives.py\u001b[0m in \u001b[0;36m_attention\u001b[0;34m(query, key, value, biases)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_no_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# [*, H, Q, C_hidden]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/model/primitives.py\u001b[0m in \u001b[0;36msoftmax_no_cast\u001b[0;34m(t, dim)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1832\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1834\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1835\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 21.05 GiB (GPU 0; 44.43 GiB total capacity; 34.71 GiB already allocated; 5.77 GiB free; 37.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictDiffs(set=\"val\"):\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders[set]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            print(\"Outputs shape: \", outputs.size())\n",
    "            print(\"Labels shape: \", labels.size())\n",
    "\n",
    "predictDiffs()\n",
    "\n",
    "diffs = np.array([0, 0.1, 0.2,-0.2, -0.8, 0.1])\n",
    "plt.hist(diffs, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('hotprot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ce9c753ae11230af47747c2525775742a5c4219355d1b58136056d9f8dd6ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
