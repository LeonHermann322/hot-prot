{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "In this notebook we classify the CT/PET scans of lung cancer cases by tumor type. \n",
    "We are doing this as a baseline task to validate our data loading pipeline.\n",
    "Most code is taken from [this tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pynvml module not found, please install pynvml'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from thermostability.thermo_dataset import ThermostabilityDataset\n",
    "\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.list_gpu_processes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining datasets (train/validation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache file:  data/train_sequences.fasta_cache.p\n",
      "Loading data from cache file:  data/eval_sequences.fasta_cache.p\n",
      "{'train': 200, 'val': 50}\n"
     ]
    }
   ],
   "source": [
    "trainSet = ThermostabilityDataset(\"data/train_sequences.fasta\", max_seq_len=200, max_ds_len=200)\n",
    "valSet = ThermostabilityDataset(\"data/eval_sequences.fasta\", max_seq_len=200, max_ds_len=50)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(trainSet, batch_size=8, shuffle=True, num_workers=4),\n",
    "    \"val\": torch.utils.data.DataLoader(valSet, batch_size=8, shuffle=True, num_workers=4)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\"train\": len(trainSet),\"val\": len(valSet)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [('MDLKQYITIVPDFPKPGIMFKDITTLMDNGPAYKYATDQIVQYAREKQIDIVVGPEARGFIIGCPVAYALGVGFAPVRKEGKLPREVVRVEYGLEYGTDVLTMHKDAIKPGQRVLITDDLLATGGTMRATIDLVEQLGGVVAGLAFLIELTELGGRKKLEGYDILTLMQF', 'MKVAEKQIEVRYAETDQMGVVYHANYLVWMEVGRTELIKQLGFHYADMEKKGIISPVVDLQVSYKKPLRYGETATVRTWIDAYDGIRVTYGYEILAPDGEVAVTGKSQHVCVKRDTFRPIVIRKYFPDWHEAYERAKR', 'MKDPRDIIKRPIITENTMNLIGQKKYTFEVDVKANKTEVKDAVEKIFGVKVAKVNIMNYKGKFKRVGRYSGYTNRRRKAIVTLTPDSKEIELFEV', 'MKHNRWASLLGLAQRAGKVVSGEELVVKEVQRGRARLVLLSQDASVNTEKKVTDKCTFYGVPLCKVPDRYVLGGAIGKDARVVVAVIDEGFARQLQTMLDRS', 'MMIRPIEVRDAENFLELCKKIDESGFMLFEPGERQTTVEQQSKSIERMLFEPNKMIFVAETENKLVGFLAVIGGDVKRNRHSANVVLEFLKTIKDRALPLNYSTKRLSGQRKLEF', 'MKEEKKILGERRRQLILQWLKESEAPLTGAELAAKTNVSRQVIVQDISLLKARNEPIIATSQGYLYLKPAEPAKTYTRTVACFHTPEQTKEELYLLVDCGVTVKDVKIEHPVYGDLTASIMVSNRLEVDQFIAKIEATKSSYLLQLTDGTHLHTLEADSPVKLDAACRALKQAGFLIEA', 'MPLHVVLYQPEIPANTGNIARTCAATDTSLHLIRPLGFSTDDKMLKRAGLDYWPYVNISYYDSLDELFARFPEGEFYFITKFGRRYYDSFDFSDTEKHIFFVFGRETTGLPKELLEANMDRCLRIPMNDKVRSLNLSNTAAILVYEALRQQRFYGLS', 'MFHPIAYRGTREENYQLVIEQLKALIAGEPNFIANLANAAALLNQFFTDINWVGFYLAEGEELVLGPFQGLPACVRIPFGKGVCGTAAAERRTVVVPDVHQFPGHIACDAASQSEIVVPLIKDGRVIGVLDIDSPVKNHFDDIDRRYLEQFASVLMSA'), tensor([[69.8016],\n",
      "        [76.5659],\n",
      "        [70.4940],\n",
      "        [72.7102],\n",
      "        [67.6693],\n",
      "        [73.5541],\n",
      "        [70.4497],\n",
      "        [83.8478]])])\n"
     ]
    }
   ],
   "source": [
    "print(next(enumerate(dataloaders[\"train\"])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HotInfer(\n",
       "  (esmfold): ESMFold(\n",
       "    (esm): ESM2(\n",
       "      (embed_tokens): Embedding(33, 2560, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (24): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (25): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (26): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (27): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (28): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (29): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (30): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (31): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (32): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (33): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (34): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (35): TransformerLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (rot_emb): RotaryEmbedding()\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (contact_head): ContactPredictionHead(\n",
       "        (regression): Linear(in_features=1440, out_features=1, bias=True)\n",
       "        (activation): Sigmoid()\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        (layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (esm_s_mlp): Sequential(\n",
       "      (0): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=2560, out_features=1024, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(23, 1024, padding_idx=0)\n",
       "    (trunk): FoldingTrunk(\n",
       "      (pairwise_positional_embedding): RelativePosition(\n",
       "        (embedding): Embedding(66, 128)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (24): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (25): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (26): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (27): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (28): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (29): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (30): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (31): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (32): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (33): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (34): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (35): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (36): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (37): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (38): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (39): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (40): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (41): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (42): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (43): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (44): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (45): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (46): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (47): TriangularSelfAttentionBlock(\n",
       "          (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (sequence_to_pair): SequenceToPair(\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (pair_to_sequence): PairToSequence(\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          )\n",
       "          (seq_attention): Attention(\n",
       "            (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (layer_norm_in): LayerNorm()\n",
       "            (layer_norm_out): LayerNorm()\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (tri_att_start): TriangleAttention(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (tri_att_end): TriangleAttentionEndingNode(\n",
       "            (layer_norm): LayerNorm()\n",
       "            (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "            (mha): Attention(\n",
       "              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (mlp_seq): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp_pair): ResidueMLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop): Dropout(p=0, inplace=False)\n",
       "          (row_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (col_drop): Dropout(\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (recycle_s_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (recycle_z_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (recycle_disto): Embedding(15, 128)\n",
       "      (structure_module): StructureModule(\n",
       "        (layer_norm_s): LayerNorm()\n",
       "        (layer_norm_z): LayerNorm()\n",
       "        (linear_in): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (ipa): InvariantPointAttention(\n",
       "          (linear_q): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (linear_kv): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (linear_q_points): Linear(in_features=384, out_features=144, bias=True)\n",
       "          (linear_kv_points): Linear(in_features=384, out_features=432, bias=True)\n",
       "          (linear_b): Linear(in_features=128, out_features=12, bias=True)\n",
       "          (linear_out): Linear(in_features=2112, out_features=384, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (softplus): Softplus(beta=1, threshold=20)\n",
       "        )\n",
       "        (ipa_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm_ipa): LayerNorm()\n",
       "        (transition): StructureModuleTransition(\n",
       "          (layers): ModuleList(\n",
       "            (0): StructureModuleTransitionLayer(\n",
       "              (linear_1): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (linear_2): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (linear_3): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (relu): ReLU()\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm()\n",
       "        )\n",
       "        (bb_update): BackboneUpdate(\n",
       "          (linear): Linear(in_features=384, out_features=6, bias=True)\n",
       "        )\n",
       "        (angle_resnet): AngleResnet(\n",
       "          (linear_in): Linear(in_features=384, out_features=128, bias=True)\n",
       "          (linear_initial): Linear(in_features=384, out_features=128, bias=True)\n",
       "          (layers): ModuleList(\n",
       "            (0): AngleResnetBlock(\n",
       "              (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (relu): ReLU()\n",
       "            )\n",
       "            (1): AngleResnetBlock(\n",
       "              (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (relu): ReLU()\n",
       "            )\n",
       "          )\n",
       "          (linear_out): Linear(in_features=128, out_features=14, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (trunk2sm_s): Linear(in_features=1024, out_features=384, bias=True)\n",
       "      (trunk2sm_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (distogram_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (ptm_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (lm_head): Linear(in_features=1024, out_features=23, bias=True)\n",
       "    (lddt_head): Sequential(\n",
       "      (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=1850, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (thermo_module_rnn): RNN(1024, 128, batch_first=True)\n",
       "  (thermo_module_regression): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thermostability.hotinfer import HotInfer\n",
    "model = HotInfer()\n",
    "model.esmfold.requires_grad_(False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "def train_model(model, criterion,optimizer , scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_epoch_loss = sys.float_info.max\n",
    "    losses = []\n",
    "    batchEnumeration = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "         \n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                #inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    \n",
    "                    loss = criterion(outputs,labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        if not torch.isnan(loss):\n",
    "                            loss.backward()\n",
    "                            threshold = 10\n",
    "                            for p in model.parameters():\n",
    "                                if p.grad != None:\n",
    "                                    if p.grad.norm() > threshold:\n",
    "                                        torch.nn.utils.clip_grad_norm_(p, threshold)\n",
    "                            optimizer.step()\n",
    "                        if torch.isnan(loss).any():\n",
    "                            print(f\"Nan loss: {torch.isnan(loss)}| Loss: {loss}| inputs: {inputs}\")\n",
    "                        \n",
    "\n",
    "                # statistics\n",
    "                batch_size = len(inputs)\n",
    "                batch_loss = loss.item() * batch_size\n",
    "                losses.append(batch_loss)\n",
    "                batchEnumeration.append(batchEnumeration[-1]+1 if len(batchEnumeration)>0 else 0)\n",
    "\n",
    "                running_loss += batch_loss\n",
    "               \n",
    "            \n",
    "                if idx % 10 == 0:\n",
    "                    batch_size = len(inputs)\n",
    "               \n",
    "                    tqdm.write(\"Epoch: [{}/{}], Batch: [{}/{}], loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        num_epochs,\n",
    "                        idx + 1,\n",
    "                        len(dataloaders[phase]),\n",
    "                        batch_loss / float(batch_size)\n",
    "                        ), end=\"\\r\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_epoch_loss:\n",
    "                best_epoch_loss = epoch_loss\n",
    "                #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_epoch_loss:4f}')\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "Epoch: [0/1], Batch: [1/25], loss: 4704.580078\r"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infering thermostability for sample 0/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([40.4280])\n",
      "Infering thermostability for sample 1/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([14.8079])\n",
      "Infering thermostability for sample 2/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([52.3495])\n",
      "Infering thermostability for sample 3/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([41.9459])\n",
      "Infering thermostability for sample 4/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([41.8849])\n",
      "Infering thermostability for sample 5/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([21.9645])\n",
      "Infering thermostability for sample 6/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([41.6771])\n",
      "Infering thermostability for sample 7/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([43.9307])\n",
      "Infering thermostability for sample 8/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([48.1022])\n",
      "Infering thermostability for sample 9/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([33.8257])\n",
      "Infering thermostability for sample 10/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([11.9578])\n",
      "Infering thermostability for sample 11/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([44.7004])\n",
      "Infering thermostability for sample 12/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([52.3142])\n",
      "Infering thermostability for sample 13/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([18.4279])\n",
      "Infering thermostability for sample 14/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([53.4864])\n",
      "Infering thermostability for sample 15/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([43.8229])\n",
      "Infering thermostability for sample 16/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([51.0027])\n",
      "Infering thermostability for sample 17/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([17.9448])\n",
      "Infering thermostability for sample 18/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([53.1549])\n",
      "Infering thermostability for sample 19/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([51.8086])\n",
      "Infering thermostability for sample 20/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([29.3117])\n",
      "Infering thermostability for sample 21/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([50.7487])\n",
      "Infering thermostability for sample 22/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([45.2723])\n",
      "Infering thermostability for sample 23/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([51.2575])\n",
      "Infering thermostability for sample 24/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([44.7799])\n",
      "Infering thermostability for sample 25/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([46.6447])\n",
      "Infering thermostability for sample 26/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([43.6656])\n",
      "Infering thermostability for sample 27/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([46.3254])\n",
      "Infering thermostability for sample 28/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([22.6844])\n",
      "Infering thermostability for sample 29/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([51.2256])\n",
      "Infering thermostability for sample 30/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([29.5997])\n",
      "Infering thermostability for sample 31/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([48.6577])\n",
      "Infering thermostability for sample 32/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([48.9908])\n",
      "Infering thermostability for sample 33/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([36.4742])\n",
      "Infering thermostability for sample 34/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([23.1746])\n",
      "Infering thermostability for sample 35/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([52.3881])\n",
      "Infering thermostability for sample 36/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([18.1336])\n",
      "Infering thermostability for sample 37/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([38.2028])\n",
      "Infering thermostability for sample 38/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([8.9676])\n",
      "Infering thermostability for sample 39/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([55.9747])\n",
      "Infering thermostability for sample 40/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([-7.3710])\n",
      "Infering thermostability for sample 41/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([49.2887])\n",
      "Infering thermostability for sample 42/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([46.0201])\n",
      "Infering thermostability for sample 43/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([49.5697])\n",
      "Infering thermostability for sample 44/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([52.7304])\n",
      "Infering thermostability for sample 45/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([45.7217])\n",
      "Infering thermostability for sample 46/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([47.1203])\n",
      "Infering thermostability for sample 47/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([48.6087])\n",
      "Infering thermostability for sample 48/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([56.5201])\n",
      "Infering thermostability for sample 49/50...\n",
      "1) any nan: False\n",
      "2) any nan: False\n",
      "3) any nan: False\n",
      "Diff:  tensor([52.0130])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  1.,  4.,  4.,  2.,  2.,  6., 16., 14.]),\n",
       " array([-7.370964  , -0.98185766,  5.4072485 , 11.796355  , 18.185461  ,\n",
       "        24.574568  , 30.963675  , 37.35278   , 43.741886  , 50.130993  ,\n",
       "        56.5201    ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predictDiffs(set=\"val\"):\n",
    "    with torch.no_grad():\n",
    "        n = len(dataloaders[set])\n",
    "        diffs = torch.tensor([])\n",
    "        for index, (inputs, labels) in enumerate(dataloaders[set]):\n",
    "            #inputs = inputs.to(device)\n",
    "            print(f\"Infering thermostability for sample {index}/{n}...\")\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _diffs = outputs.squeeze(1).sub(labels.squeeze(1)).cpu()\n",
    "            diffs = torch.cat((diffs, _diffs))\n",
    "            print(\"Diff: \", _diffs)\n",
    "    return diffs\n",
    "diffs = predictDiffs()\n",
    "\n",
    "#diffs = np.array([0, 0.1, 0.2,-0.2, -0.8, 0.1])\n",
    "plt.hist(diffs, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('hotprot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ce9c753ae11230af47747c2525775742a5c4219355d1b58136056d9f8dd6ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
