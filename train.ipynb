{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "In this notebook we classify the CT/PET scans of lung cancer cases by tumor type. \n",
    "We are doing this as a baseline task to validate our data loading pipeline.\n",
    "Most code is taken from [this tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "context has already been set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2650254/3830349910.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_gpu_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_start_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtelegramBot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTelegramBot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36mset_start_method\u001b[0;34m(self, method, force)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_start_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actual_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'context has already been set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actual_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: context has already been set"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from thermostability.thermo_pregenerated_dataset import ThermostabilityPregeneratedDataset\n",
    "from util.telegram import TelegramBot\n",
    "\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.list_gpu_processes()\n",
    "\n",
    "telegramBot = TelegramBot()\n",
    "telegramBot.enabled=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining datasets (train/validation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 34234, 'val': 3356}\n"
     ]
    }
   ],
   "source": [
    "trainSet = ThermostabilityPregeneratedDataset(\"data/s_s/train/\")\n",
    "valSet = ThermostabilityPregeneratedDataset(\"data/s_s/val/\")\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(trainSet, batch_size=16, shuffle=True, num_workers=4),\n",
    "    \"val\": torch.utils.data.DataLoader(valSet, batch_size=16, shuffle=True, num_workers=4)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\"train\": len(trainSet),\"val\": len(valSet)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/dhc/home/tobias.fiedler/workspaces/hot-prot/thermostability/thermo_pregenerated_dataset.py\", line 32, in __getitem__\n    s_s = pickle.load(f)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/storage.py\", line 222, in _load_from_bytes\n    return torch.load(io.BytesIO(b))\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 713, in load\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 930, in _legacy_load\n    result = unpickler.load()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 876, in persistent_load\n    wrap_storage=restore_location(obj, location),\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n    result = fn(storage, location)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 154, in _cuda_deserialize\n    with torch.cuda.device(device):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 275, in __enter__\n    self.prev_idx = torch.cuda.current_device()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 482, in current_device\n    _lazy_init()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 208, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. To use CUDA with \"\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2650254/2819047035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/dhc/home/tobias.fiedler/workspaces/hot-prot/thermostability/thermo_pregenerated_dataset.py\", line 32, in __getitem__\n    s_s = pickle.load(f)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/storage.py\", line 222, in _load_from_bytes\n    return torch.load(io.BytesIO(b))\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 713, in load\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 930, in _legacy_load\n    result = unpickler.load()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 876, in persistent_load\n    wrap_storage=restore_location(obj, location),\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n    result = fn(storage, location)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 154, in _cuda_deserialize\n    with torch.cuda.device(device):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 275, in __enter__\n    self.prev_idx = torch.cuda.current_device()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 482, in current_device\n    _lazy_init()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 208, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. To use CUDA with \"\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "print(next(enumerate(dataloaders[\"train\"])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HotInferPregenerated(\n",
       "  (thermo_module_rnn): RNN(1024, 128, batch_first=True)\n",
       "  (thermo_module_regression): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thermostability.hotinfer_pregenerated import HotInferPregenerated\n",
    "model = HotInferPregenerated()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "def train_model(model, criterion,optimizer , scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    telegramBot.send_telegram(f\"===> Starting training ({num_epochs} epochs, {len(trainSet)} train samples, {len(valSet)} val samples)\")\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_epoch_loss = sys.float_info.max\n",
    "    losses = []\n",
    "    batchEnumeration = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "         \n",
    "            sliding_loss = 0.0\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                #inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    \n",
    "                    loss = criterion(outputs,labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        if not torch.isnan(loss):\n",
    "                            loss.backward()\n",
    "                            threshold = 10\n",
    "                            for p in model.parameters():\n",
    "                                if p.grad != None:\n",
    "                                    if p.grad.norm() > threshold:\n",
    "                                        torch.nn.utils.clip_grad_norm_(p, threshold)\n",
    "                            optimizer.step()\n",
    "                        if torch.isnan(loss).any():\n",
    "                            print(f\"Nan loss: {torch.isnan(loss)}| Loss: {loss}| inputs: {inputs}\")\n",
    "                        \n",
    "\n",
    "                # statistics\n",
    "                batch_size = len(inputs)\n",
    "                batch_loss = loss.item() * batch_size\n",
    "                losses.append(batch_loss)\n",
    "                batchEnumeration.append(batchEnumeration[-1]+1 if len(batchEnumeration)>0 else 0)\n",
    "\n",
    "                running_loss += batch_loss\n",
    "                sliding_loss += batch_loss\n",
    "           \n",
    "                if idx % 10 == 0:\n",
    "                    tqdm.write(\"Epoch: [{}/{}], Batch: [{}/{}], loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        num_epochs,\n",
    "                        idx + 1,\n",
    "                        len(dataloaders[phase]),\n",
    "                        batch_loss / float(batch_size)\n",
    "                        ), end=\"\\r\")\n",
    "                \n",
    "                telegramFrequency = 10\n",
    "                if idx % telegramFrequency == telegramFrequency-1:\n",
    "                    telegramBot.send_telegram(\"Epoch: [{}/{}], Batch: [{}/{}], Batch loss: {:.6f}, Total Avg Epoch loss: {:.6f}, Avg loss last {} epochs: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        num_epochs,\n",
    "                        idx + 1,\n",
    "                        len(dataloaders[phase]),\n",
    "                        batch_loss / float(batch_size),\n",
    "                        (running_loss/batch_size)/(idx+1),\n",
    "                        telegramFrequency,\n",
    "                        (sliding_loss/batch_size)/telegramFrequency\n",
    "                        ))\n",
    "                    sliding_loss = 0.    \n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_epoch_loss:\n",
    "                best_epoch_loss = epoch_loss\n",
    "                #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    telegramBot.send_telegram(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_epoch_loss:4f}')\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "Caught RuntimeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/dhc/home/tobias.fiedler/workspaces/hot-prot/thermostability/thermo_pregenerated_dataset.py\", line 32, in __getitem__\n",
      "    s_s = pickle.load(f)\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/storage.py\", line 222, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 713, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 930, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 876, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 154, in _cuda_deserialize\n",
      "    with torch.cuda.device(device):\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 275, in __enter__\n",
      "    self.prev_idx = torch.cuda.current_device()\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 482, in current_device\n",
      "    _lazy_init()\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 208, in _lazy_init\n",
      "    \"Cannot re-initialize CUDA in forked subprocess. To use CUDA with \"\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                        num_epochs=1)\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    telegramBot.send_telegram(f\"Training failed with error message: {str(e)}\")                         "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f692e589a70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "      File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f692e589a70>\n",
      "Traceback (most recent call last):\n",
      "      File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    if w.is_alive():\n",
      "self._shutdown_workers()  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "\n",
      "      File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    AssertionErrorif w.is_alive():: \n",
      "can only test a child process  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/dhc/home/tobias.fiedler/workspaces/hot-prot/thermostability/thermo_pregenerated_dataset.py\", line 32, in __getitem__\n    s_s = pickle.load(f)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/storage.py\", line 222, in _load_from_bytes\n    return torch.load(io.BytesIO(b))\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 713, in load\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 930, in _legacy_load\n    result = unpickler.load()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 876, in persistent_load\n    wrap_storage=restore_location(obj, location),\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n    result = fn(storage, location)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 154, in _cuda_deserialize\n    with torch.cuda.device(device):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 275, in __enter__\n    self.prev_idx = torch.cuda.current_device()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 482, in current_device\n    _lazy_init()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 208, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. To use CUDA with \"\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2650254/393846584.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Diff: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_diffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdiffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictDiffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#diffs = np.array([0, 0.1, 0.2,-0.2, -0.8, 0.1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2650254/393846584.py\u001b[0m in \u001b[0;36mpredictDiffs\u001b[0;34m(set)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;31m#inputs = inputs.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Infering thermostability for sample {index}/{n}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/dhc/home/tobias.fiedler/workspaces/hot-prot/thermostability/thermo_pregenerated_dataset.py\", line 32, in __getitem__\n    s_s = pickle.load(f)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/storage.py\", line 222, in _load_from_bytes\n    return torch.load(io.BytesIO(b))\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 713, in load\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 930, in _legacy_load\n    result = unpickler.load()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 876, in persistent_load\n    wrap_storage=restore_location(obj, location),\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n    result = fn(storage, location)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/serialization.py\", line 154, in _cuda_deserialize\n    with torch.cuda.device(device):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 275, in __enter__\n    self.prev_idx = torch.cuda.current_device()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 482, in current_device\n    _lazy_init()\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 208, in _lazy_init\n    \"Cannot re-initialize CUDA in forked subprocess. To use CUDA with \"\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def predictDiffs(set=\"val\"):\n",
    "    with torch.no_grad():\n",
    "        n = len(dataloaders[set])\n",
    "        diffs = torch.tensor([])\n",
    "        for index, (inputs, labels) in enumerate(dataloaders[set]):\n",
    "            #inputs = inputs.to(device)\n",
    "            print(f\"Infering thermostability for sample {index}/{n}...\")\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _diffs = outputs.squeeze(1).sub(labels.squeeze(1)).cpu()\n",
    "            diffs = torch.cat((diffs, _diffs))\n",
    "            print(\"Diff: \", _diffs)\n",
    "    return diffs\n",
    "diffs = predictDiffs()\n",
    "\n",
    "#diffs = np.array([0, 0.1, 0.2,-0.2, -0.8, 0.1])\n",
    "plt.title(\"Differences predicted <-> actual thermostability\")\n",
    "plt.hist(diffs, 10)\n",
    "resultsDir = \"results\"\n",
    "now = datetime.now()\n",
    "time = now.strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "os.makedirs(resultsDir, exist_ok=True)\n",
    "histFile = f\"results/{time}_diffs.png\"\n",
    "plt.savefig(histFile)\n",
    "telegramBot.send_photo(histFile, f\"Differences predicted <-> actual thermostability at {time}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    modelPath = os.path.join(resultsDir, f\"{time}_model.pth\")\n",
    "    torch.save(model, modelPath)\n",
    "    telegramBot.send_telegram(f\"Model saved at {modelPath}\")\n",
    "except Exception as e:\n",
    "    telegramBot.send_telegram(f\"Saving model failed for reason: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('hotprot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ce9c753ae11230af47747c2525775742a5c4219355d1b58136056d9f8dd6ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
