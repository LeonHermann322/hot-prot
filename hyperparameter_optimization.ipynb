{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pynvml module not found, please install pynvml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import time\n",
    "import copy\n",
    "from torch import nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import lr_scheduler\n",
    "from pathlib import Path\n",
    "#import mlflow\n",
    "from thermostability.thermo_dataset import ThermostabilityDataset\n",
    "from thermostability.thermo_pregenerated_dataset import ThermostabilityPregeneratedDataset\n",
    "from thermostability.hotinfer_pregenerated import HotInferPregenerated\n",
    "from thermostability.hotinfer import HotInfer\n",
    "import wandb\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.list_gpu_processes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reduced: 367394 --> 36739\n",
      "Dataset reduced: 35170 --> 3517\n"
     ]
    }
   ],
   "source": [
    "def prep_minimal_dataset(src_path: Path, dst_path: Path):\n",
    "    minimized_data = []\n",
    "    reduction = 10\n",
    "    with open(src_path) as file:\n",
    "        data = file.readlines()\n",
    "        file_len = len(data)\n",
    "        minimized_data = data[:round(len(data) / reduction)]\n",
    "        print(f'Dataset reduced: {file_len} --> {len(minimized_data)}')\n",
    "        \n",
    "    with open(dst_path, 'w') as file:\n",
    "        file.writelines(minimized_data)\n",
    "            \n",
    "prep_minimal_dataset(Path('data/train_sequences.fasta'), Path('data/minimal_train_sequences.fasta'))\n",
    "prep_minimal_dataset(Path('data/eval_sequences.fasta'), Path('data/minimal_eval_sequences.fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading line 36700\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_ds \u001b[39m=\u001b[39m ThermostabilityDataset(\u001b[39m'\u001b[39;49m\u001b[39mdata/minimal_train_sequences.fasta\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m eval_ds \u001b[39m=\u001b[39m ThermostabilityDataset(\u001b[39m'\u001b[39m\u001b[39mdata/minimal_eval_sequences.fasta\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m dataloaders \u001b[39m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_ds, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m),\n\u001b[0;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(eval_ds, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m      7\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\lherm\\Desktop\\hot-prot\\thermostability\\thermo_dataset.py:38\u001b[0m, in \u001b[0;36mThermostabilityDataset.__init__\u001b[1;34m(self, file_path, use_cache, max_seq_len, max_ds_len)\u001b[0m\n\u001b[0;32m     34\u001b[0m             sequences\u001b[39m.\u001b[39mappend(sequence)\n\u001b[0;32m     36\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m: sequences, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m: melting_points}\n\u001b[1;32m---> 38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthermo_dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data)\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n\u001b[0;32m     40\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m( cache_file, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m ) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\lherm\\Desktop\\hot-prot\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lherm\\Desktop\\hot-prot\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\lherm\\Desktop\\hot-prot\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\lherm\\Desktop\\hot-prot\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    665\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    669\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "train_ds = ThermostabilityDataset('data/minimal_train_sequences.fasta')\n",
    "eval_ds = ThermostabilityDataset('data/minimal_eval_sequences.fasta')\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4),\n",
    "    \"val\": torch.utils.data.DataLoader(eval_ds, batch_size=1, shuffle=True, num_workers=4)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\"train\": len(train_ds),\"val\": len(eval_ds)}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "def train_model(model, optimizer, criterion, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_epoch_loss = sys.float_info.max\n",
    "    losses = []\n",
    "    batchEnumeration = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "         \n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                if torch.isnan(inputs).any():\n",
    "                    print(\"#########################################################################################\\n################\")\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    loss = criterion(outputs,labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        if not torch.isnan(loss):\n",
    "                            loss.backward()\n",
    "                            threshold = 10000\n",
    "                            for p in model.parameters():\n",
    "                                if p.grad != None:\n",
    "                                    if p.grad.norm() > threshold:\n",
    "                                        torch.nn.utils.clip_grad_norm_(p, threshold)\n",
    "                            optimizer.step()\n",
    "                        if torch.isnan(loss).any():\n",
    "                            print(f\"Nan loss: {torch.isnan(loss)}| Loss: {loss}| inputs: {inputs}\")\n",
    "\n",
    "                # statistics\n",
    "                batch_size = inputs.size(0)\n",
    "                batch_loss = loss.item() * batch_size\n",
    "                losses.append(batch_loss)\n",
    "                batchEnumeration.append(batchEnumeration[-1]+1 if len(batchEnumeration)>0 else 0)\n",
    "\n",
    "                running_loss += batch_loss\n",
    "               \n",
    "            \n",
    "                if idx % 10 == 0:\n",
    "                    batch_size = inputs.size(0)\n",
    "                    tqdm.write(\"Epoch: [{}/{}], Batch: [{}/{}], train accuracy: {:.6f}, loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        num_epochs,\n",
    "                        idx + 1,\n",
    "                        len(dataloaders[phase]),\n",
    "                        batch_loss / float(batch_size), running_loss / (idx + 1)\n",
    "                        ), end=\"\\r\")\n",
    "                    \n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_epoch_loss:\n",
    "                best_epoch_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_epoch_loss:4f}')\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "# mlflc = MLflowCallback(\n",
    "#     tracking_uri=YOUR_TRACKING_URI,\n",
    "#     metric_name=\"metric_score\"\n",
    "#)\n",
    "\n",
    "def optimize_thermostability(trial):    \n",
    "    params = {\n",
    "        'model_learning_rate': trial.suggest_float('model_learning_rate', 0.001, 0.501, step=0.05),\n",
    "        'model_hidden_units': trial.suggest_int('model_hidden_units', 64, 640, step=64),\n",
    "        'model_hidden_layers': trial.suggest_int('model_hidden_layers', 1, 4, step=1)\n",
    "    }\n",
    "    #wandb.init(project=\"HotProt\", entity=\"7-vs-capsule\")\n",
    "    #\n",
    "    #wandb.config = params\n",
    "    model = HotInferPregenerated(\n",
    "        params['model_hidden_units'],\n",
    "        params['model_hidden_layers'],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer_ft = torch.optim.SGD(model.parameters(), lr=params['model_learning_rate'], momentum=0.9)\n",
    "    \n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "    model, score = train_model(model, optimizer_ft, criterion, exp_lr_scheduler, num_epochs=10)\n",
    "\n",
    "    #mlflow.log_params(params)\n",
    "\n",
    "    #wandb.log({\"score\": score})\n",
    "    #candidate_model_uri = mlflow.pytorch.log_model(model).model_uri\n",
    "    #mlflow.evaluate(model=candidate_model_uri, data=eval_data, targets=\"label\", model_type=\"regressor\")\n",
    "    #mlflow.log_metric(\"score\", score)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-20 15:13:11,437]\u001b[0m A new study created in memory with name: thermostability-hyperparameter-optimization\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 11522075641428048796712257907064832.0000792501685808878958323491739271168.000000, loss: 11519619703588062500052228741529600.000000\n",
      "val Loss: 11792645857543793176871506785861632.000011792501685808878958323491739271168.000000, loss: 11792647166873375357707317059518464.000000\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 11792645857543793176871506785861632.0000792501685808878958323491739271168.000000, loss: 11792647166873375357707317059518464.000000\n",
      "val Loss: 11792645857543793176871506785861632.000011792501685808878958323491739271168.000000, loss: 11792647176866836701878217877749760.000000\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 11792645857543793176871506785861632.0000792501685808878958323491739271168.000000, loss: 11792644803419738638251658376642560.000000\n",
      "val Loss: 11792645857543793176871506785861632.000011792501685808878958323491739271168.000000, loss: 11792618670518000813324728784650240.000000\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 11792117026890172469706175551635456.0000790409567142486665658912219332608.000000, loss: 11792132516099626368420677353996288.000000\n",
      "val Loss: 11790555231833088557523443973619712.000011790409567142486665658912219332608.000000, loss: 11790556554721297416167852687228928.000000\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 11789314066960002591273615739584512.0000788352110797194363642029874872320.000000, loss: 11789322803191551810602099374292992.000000\n",
      "val Loss: 11788497078527555189301879062396928.000011788352110797194363642029874872320.000000, loss: 11788498395086155812364421466947584.000000\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 11788497078527555189301879062396928.0000788352110797194363642029874872320.000000, loss: 11788498445053462533218925558104064.000000\n",
      "val Loss: 11788497078527555189301879062396928.000011788352110797194363642029874872320.000000, loss: 11788498395086155812364421466947584.000000\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 11788497078527555189301879062396928.0000788352110797194363642029874872320.000000, loss: 11788498395086155812364421466947584.000000\n",
      "val Loss: 11788497078527555189301879062396928.000011788352110797194363642029874872320.000000, loss: 11788498395086155812364421466947584.000000\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 11788497078527555189301879062396928.0000788352110797194363642029874872320.000000, loss: 11788492362782976132615487161892864.000000\n",
      "val Loss: 11788497078527555189301879062396928.000011788352110797194363642029874872320.000000, loss: 11788496051619441781250564420534272.000000\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 11788497078527555189301879062396928.0000788352110797194363642029874872320.000000, loss: 11788498395086155812364421466947584.000000\n",
      "val Loss: 11788497078527555189301879062396928.000011788352110797194363642029874872320.000000, loss: 11788497297054577651219767236755456.000000\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 11788497078527555189301879062396928.0000788352110797194363642029874872320.000000, loss: 11788498395086155812364421466947584.000000\n",
      "Epoch: [9/10], Batch: [981/1000], train accuracy: 11788352110797194363642029874872320.000000, loss: 11788497099949849009978140688121856.000000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-20 15:21:23,246]\u001b[0m Trial 0 finished with value: 1.1788497078527555e+34 and parameters: {'model_learning_rate': 0.15100000000000002, 'model_hidden_units': 384, 'model_hidden_layers': 4}. Best is trial 0 with value: 1.1788497078527555e+34.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 11788497078527555189301879062396928.000011788352110797194363642029874872320.000000, loss: 11788495636890790233550657429700608.000000\n",
      "\n",
      "Training complete in 8m 12s\n",
      "Best val Acc: 11788497078527555189301879062396928.000000\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 20467709763435774269718528.0000uracy: 590.273254, loss: 20653592092266168775081984.0000000000000823979008.000000\n",
      "val Loss: 403.5739ch: [991/1000], train accuracy: 2.620529, loss: 404.085692235\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 603.6627: [991/1000], train accuracy: 224.120117, loss: 605.9560217\n",
      "val Loss: 233.0333ch: [991/1000], train accuracy: 0.703929, loss: 234.205741226\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 570.1187: [991/1000], train accuracy: 79.076721, loss: 573.79974040\n",
      "val Loss: 188.2541ch: [991/1000], train accuracy: 551.766724, loss: 188.655888\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 587.1956: [991/1000], train accuracy: 1527.239380, loss: 589.274282\n",
      "val Loss: 152.1965ch: [991/1000], train accuracy: 12.147702, loss: 151.8069110\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 641.5493: [991/1000], train accuracy: 449.898376, loss: 645.9486403\n",
      "val Loss: 135.5693ch: [991/1000], train accuracy: 0.059810, loss: 135.93096821\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 542.7837: [991/1000], train accuracy: 2331.718506, loss: 535.918256\n",
      "val Loss: 1083.4064h: [991/1000], train accuracy: 578.873230, loss: 1088.7164096\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 583.2311: [991/1000], train accuracy: 483.296631, loss: 586.3734818\n",
      "val Loss: 275.3517ch: [991/1000], train accuracy: 84.455444, loss: 275.32056633\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 160.4474: [991/1000], train accuracy: 72.584496, loss: 160.27641566\n",
      "val Loss: 126.2332ch: [991/1000], train accuracy: 0.135331, loss: 126.81209258\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 163.7661: [991/1000], train accuracy: 518.302673, loss: 163.849254\n",
      "val Loss: 127.9858ch: [991/1000], train accuracy: 8.843808, loss: 128.18780495\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 164.9080: [991/1000], train accuracy: 174.715881, loss: 165.1299970\n",
      "Epoch: [9/10], Batch: [991/1000], train accuracy: 8.526612, loss: 146.58007010\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-20 15:25:36,088]\u001b[0m Trial 1 finished with value: 126.23322334720405 and parameters: {'model_learning_rate': 0.30100000000000005, 'model_hidden_units': 512, 'model_hidden_layers': 1}. Best is trial 1 with value: 126.23322334720405.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 146.0555\n",
      "\n",
      "Training complete in 4m 13s\n",
      "Best val Acc: 126.233223\n",
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 11744284195693917551531571281920.0000 12074347305087164910820934025216.000000, loss: 11741286649796298367787383914496.000000\n",
      "val Loss: 12072330543602812728652909772800.0000y: 12074347305087164910820934025216.000000, loss: 12072312227908200949620194410496.000000\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 12072330543602812728652909772800.0000 12074347305087164910820934025216.000000, loss: 12072312227908200949620194410496.000000\n",
      "val Loss: 12072330543602812728652909772800.0000y: 12074347305087164910820934025216.000000, loss: 12072312227908200949620194410496.000000\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 12072330543602812728652909772800.0000 12074347305087164910820934025216.000000, loss: 12072312227908200949620194410496.000000\n",
      "val Loss: 12072330543602812728652909772800.0000y: 12074347305087164910820934025216.000000, loss: 12072312227908200949620194410496.000000\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 12072330543602812728652909772800.0000 12074347305087164910820934025216.000000, loss: 12072312227908200949620194410496.000000\n",
      "val Loss: 12072330543602812728652909772800.0000y: 12074347305087164910820934025216.000000, loss: 12072312227908200949620194410496.000000\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "Epoch: [4/10], Batch: [721/1000], train accuracy: 12074347305087164910820934025216.000000, loss: 12071550132432030144655295250432.000000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-01-20 15:26:27,741]\u001b[0m Trial 2 failed with parameters: {'model_learning_rate': 0.251, 'model_hidden_units': 64, 'model_hidden_layers': 2} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_658976/482875446.py\", line 28, in optimize_thermostability\n",
      "    model, score = train_model(model, optimizer_ft, criterion, exp_lr_scheduler, num_epochs=10)\n",
      "  File \"/tmp/ipykernel_658976/2957317812.py\", line 47, in train_model\n",
      "    loss.backward()\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 175, in backward\n",
      "    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-01-20 15:26:27,749]\u001b[0m Trial 2 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_658976/201081537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# minimize or maximize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"thermostability-hyperparameter-optimization\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# maximise the score during tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize_thermostability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run the objective function 100 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print the best performing pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         )\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_658976/482875446.py\u001b[0m in \u001b[0;36moptimize_thermostability\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#mlflow.log_params(params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_658976/2957317812.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                             \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# minimize or maximize\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"thermostability-hyperparameter-optimization\") # maximise the score during tuning\n",
    "study.optimize(optimize_thermostability, n_trials=100) # run the objective function 100 times\n",
    "\n",
    "print(study.best_trial) # print the best performing pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d305dbf6898f7d437d374baf00ea2ffa05fed7e2919e47a59f2516569616bb66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
