{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pynvml module not found, please install pynvml'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import time\n",
    "import copy\n",
    "from torch import nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import lr_scheduler\n",
    "from pathlib import Path\n",
    "#import mlflow\n",
    "from thermostability.thermo_dataset import ThermostabilityDataset\n",
    "from thermostability.thermo_pregenerated_dataset import ThermostabilityPregeneratedDataset\n",
    "from thermostability.hotinfer_pregenerated import HotInferPregenerated\n",
    "from thermostability.hotinfer import HotInfer\n",
    "import wandb\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.list_gpu_processes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ThermostabilityPregeneratedDataset('data/s_s/train', limit=1000)\n",
    "eval_ds = ThermostabilityPregeneratedDataset('data/s_s/train', limit=1000)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4),\n",
    "    \"val\": torch.utils.data.DataLoader(eval_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\"train\": len(train_ds),\"val\": len(eval_ds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "def train_model(model, optimizer, criterion, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_epoch_loss = sys.float_info.max\n",
    "    losses = []\n",
    "    batchEnumeration = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "         \n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    loss = criterion(outputs,labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                batch_size = inputs.size(0)\n",
    "                batch_loss = loss.item() * batch_size\n",
    "                losses.append(batch_loss)\n",
    "                batchEnumeration.append(batchEnumeration[-1]+1 if len(batchEnumeration)>0 else 0)\n",
    "\n",
    "                running_loss += batch_loss\n",
    "               \n",
    "            \n",
    "                if idx % 10 == 0:\n",
    "                    batch_size = inputs.size(0)\n",
    "                    tqdm.write(\"Epoch: [{}/{}], Batch: [{}/{}], train accuracy: {:.6f}, loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        num_epochs,\n",
    "                        idx + 1,\n",
    "                        len(dataloaders[phase]),\n",
    "                        batch_loss / float(batch_size)\n",
    "                        ), end=\"\\r\")\n",
    "                    \n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_epoch_loss:\n",
    "                best_epoch_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_epoch_loss:4f}')\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "# mlflc = MLflowCallback(\n",
    "#     tracking_uri=YOUR_TRACKING_URI,\n",
    "#     metric_name=\"metric_score\"\n",
    "#)\n",
    "\n",
    "def optimize_thermostability(trial):    \n",
    "    params = {\n",
    "        'model_learning_rate': trial.suggest_float('model_learning_rate', 0.001, 0.501, step=0.05),\n",
    "        'model_hidden_units': trial.suggest_int('model_hidden_units', 64, 640, step=64),\n",
    "        'model_hidden_layers': trial.suggest_int('model_hidden_layers', 1, 4, step=1)\n",
    "    }\n",
    "    #wandb.init(project=\"HotProt\", entity=\"7-vs-capsule\")\n",
    "    #\n",
    "    #wandb.config = params\n",
    "    model = HotInferPregenerated(\n",
    "        params['model_hidden_units'],\n",
    "        params['model_hidden_layers'],\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer_ft = torch.optim.SGD(model.parameters(), lr=params['model_learning_rate'], momentum=0.9)\n",
    "    \n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "    model, score = train_model(model, optimizer_ft, criterion, exp_lr_scheduler, num_epochs=10)\n",
    "\n",
    "    #mlflow.log_params(params)\n",
    "\n",
    "    #wandb.log({\"score\": score})\n",
    "    #candidate_model_uri = mlflow.pytorch.log_model(model).model_uri\n",
    "    #mlflow.evaluate(model=candidate_model_uri, data=eval_data, targets=\"label\", model_type=\"regressor\")\n",
    "    #mlflow.log_metric(\"score\", score)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-20 14:11:39,881]\u001b[0m A new study created in memory with name: thermostability-hyperparameter-optimization\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-01-20 14:11:40,970]\u001b[0m Trial 0 failed with parameters: {'model_learning_rate': 0.35100000000000003, 'model_hidden_units': 256, 'model_hidden_layers': 1} because of the following error: RuntimeError('Caught RuntimeError in DataLoader worker process 0.\\nOriginal Traceback (most recent call last):\\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\\n    data = fetcher.fetch(index)\\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\\n    return self.collate_fn(data)\\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in default_collate\\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in <listcomp>\\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 141, in default_collate\\n    return torch.stack(batch, 0, out=out)\\nRuntimeError: stack expects each tensor to be equal size, but got [299, 1024] at entry 0 and [333, 1024] at entry 1\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_364330/2478914144.py\", line 27, in optimize_thermostability\n",
      "    model, score = train_model(model, optimizer_ft, criterion, exp_lr_scheduler, num_epochs=10)\n",
      "  File \"/tmp/ipykernel_364330/3755756128.py\", line 27, in train_model\n",
      "    for idx, (inputs, labels) in enumerate(dataloaders[phase]):\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 681, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1376, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1402, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/_utils.py\", line 461, in reraise\n",
      "    raise exception\n",
      "RuntimeError: Caught RuntimeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 141, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "RuntimeError: stack expects each tensor to be equal size, but got [299, 1024] at entry 0 and [333, 1024] at entry 1\n",
      "\n",
      "\u001b[33m[W 2023-01-20 14:11:41,021]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in default_collate\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in <listcomp>\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 141, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [299, 1024] at entry 0 and [333, 1024] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_364330/201081537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# minimize or maximize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"thermostability-hyperparameter-optimization\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# maximise the score during tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize_thermostability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run the objective function 100 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print the best performing pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         )\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_364330/2478914144.py\u001b[0m in \u001b[0;36moptimize_thermostability\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#mlflow.log_params(params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_364330/3755756128.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in default_collate\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in <listcomp>\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/dhc/home/tobias.fiedler/conda3/envs/hotprot/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 141, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [299, 1024] at entry 0 and [333, 1024] at entry 1\n"
     ]
    }
   ],
   "source": [
    "# minimize or maximize\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"thermostability-hyperparameter-optimization\") # maximise the score during tuning\n",
    "study.optimize(optimize_thermostability, n_trials=100) # run the objective function 100 times\n",
    "\n",
    "print(study.best_trial) # print the best performing pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotprot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ce9c753ae11230af47747c2525775742a5c4219355d1b58136056d9f8dd6ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
