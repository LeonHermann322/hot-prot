{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from thermostability.thermo_dataset import ThermostabilityDataset\n",
    "from util.telegram import TelegramBot\n",
    "\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.list_gpu_processes()\n",
    "\n",
    "telegramBot = TelegramBot()\n",
    "telegramBot.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from cache file:  data/train_sequences.fasta_v2_cache.p\n",
      "Loading data from cache file:  data/eval_sequences.fasta_v2_cache.p\n",
      "{'train': 15685, 'val': 995}\n"
     ]
    }
   ],
   "source": [
    "trainSet = ThermostabilityDataset(\"data/train_sequences.fasta\", max_seq_len=700, once_occuring_seq_only=True)\n",
    "valSet = ThermostabilityDataset(\"data/eval_sequences.fasta\", max_seq_len=700, once_occuring_seq_only=True)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(trainSet, batch_size=2, shuffle=False, num_workers=4),\n",
    "    \"val\": torch.utils.data.DataLoader(valSet, batch_size=2, shuffle=False, num_workers=4)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\"train\": len(trainSet),\"val\": len(valSet)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMFold(\n",
       "  (esm): ESM2(\n",
       "    (embed_tokens): Embedding(33, 2560, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (12): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (13): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (14): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (15): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (16): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (17): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (18): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (19): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (20): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (21): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (22): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (23): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (24): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (25): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (26): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (27): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (28): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (29): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (30): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (31): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (32): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (33): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (34): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (35): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (contact_head): ContactPredictionHead(\n",
       "      (regression): Linear(in_features=1440, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "      (layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (esm_s_mlp): Sequential(\n",
       "    (0): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=2560, out_features=1024, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(23, 1024, padding_idx=0)\n",
       "  (trunk): FoldingTrunk(\n",
       "    (pairwise_positional_embedding): RelativePosition(\n",
       "      (embedding): Embedding(66, 128)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (28): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (29): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (30): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (31): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (32): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (33): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (34): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (35): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (36): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (37): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (38): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (39): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (40): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (41): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (42): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (43): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (44): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (45): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (46): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (47): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (recycle_s_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (recycle_z_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (recycle_disto): Embedding(15, 128)\n",
       "    (structure_module): StructureModule(\n",
       "      (layer_norm_s): LayerNorm()\n",
       "      (layer_norm_z): LayerNorm()\n",
       "      (linear_in): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (ipa): InvariantPointAttention(\n",
       "        (linear_q): Linear(in_features=384, out_features=192, bias=True)\n",
       "        (linear_kv): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (linear_q_points): Linear(in_features=384, out_features=144, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=384, out_features=432, bias=True)\n",
       "        (linear_b): Linear(in_features=128, out_features=12, bias=True)\n",
       "        (linear_out): Linear(in_features=2112, out_features=384, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (ipa_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm_ipa): LayerNorm()\n",
       "      (transition): StructureModuleTransition(\n",
       "        (layers): ModuleList(\n",
       "          (0): StructureModuleTransitionLayer(\n",
       "            (linear_1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (linear_2): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (linear_3): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm()\n",
       "      )\n",
       "      (bb_update): BackboneUpdate(\n",
       "        (linear): Linear(in_features=384, out_features=6, bias=True)\n",
       "      )\n",
       "      (angle_resnet): AngleResnet(\n",
       "        (linear_in): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (linear_initial): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): AngleResnetBlock(\n",
       "            (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): AngleResnetBlock(\n",
       "            (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (linear_out): Linear(in_features=128, out_features=14, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (trunk2sm_s): Linear(in_features=1024, out_features=384, bias=True)\n",
       "    (trunk2sm_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (distogram_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (ptm_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lm_head): Linear(in_features=1024, out_features=23, bias=True)\n",
       "  (lddt_head): Sequential(\n",
       "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=1850, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import esm\n",
    "import pickle\n",
    "import os\n",
    "esmfold = esm.pretrained.esmfold_v1()\n",
    "esmfold.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At batch 0/15685\n",
      "At batch 1/15685\n",
      "At batch 2/15685\n",
      "At batch 3/15685\n",
      "Predicting\n",
      "At batch 4/15685\n",
      "Predicting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_954375/2988585983.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mgenerateRepresenations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;31m# except Exception as e:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# print(\"Exception raised: \", e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_954375/2988585983.py\u001b[0m in \u001b[0;36mgenerateRepresenations\u001b[0;34m(set)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0malreadyPredicted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predicting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mesm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesmfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0ms_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s_s\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m#s_z = esm_output[\"s_z\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/esmfold.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, sequences, residx, masking_pattern, num_recycles, residue_index_offset, chain_linker)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mresidx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mmasking_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasking_pattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mnum_recycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_recycles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/esmfold.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, aa, mask, residx, masking_pattern, num_recycles)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         structure: dict = self.trunk(\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0ms_s_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_recycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_recycles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         )\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Documenting what we expect:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/trunk.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles)\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mrecycle_z\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecycle_disto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecycle_bins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0ms_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrunk_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_s_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecycle_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecycle_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;31m# === Structure module ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/trunk.py\u001b[0m in \u001b[0;36mtrunk_iter\u001b[0;34m(s, z, residx, mask)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidue_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/tri_self_attn_block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequence_state, pairwise_state, mask, chunk_size, **_TriangularSelfAttentionBlock__kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m         pairwise_state = pairwise_state + self.col_drop(\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtri_mul_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairwise_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtri_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         )\n\u001b[1;32m    150\u001b[0m         pairwise_state = pairwise_state + self.row_drop(\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/model/triangular_multiplicative_update.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, mask, inplace_safe, _add_with_inplace, _inplace_chunk_size)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_b_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_b_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_projections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "rootDir = \"data/s_s/\"\n",
    "\n",
    "\n",
    "def generateRepresenations(set):\n",
    "    timeStart = time.time()\n",
    "    telegramBot.send_telegram(f\"Generating s_s representations for set {set}...\")\n",
    "    setDir= os.path.join(rootDir, set)\n",
    "    os.makedirs(setDir, exist_ok=True)\n",
    "    response = telegramBot.send_telegram(f\"Generating first batch...\")\n",
    "    messageId = response[\"result\"][\"message_id\"]\n",
    "\n",
    "    labelsFilePath = os.path.join(setDir, \"labels.csv\")\n",
    "    if not os.path.exists(labelsFilePath):\n",
    "        with open(labelsFilePath,\"w\") as csv:\n",
    "            csv.write(f\"filename, thermostability, sequence\\n\") \n",
    "    batchesPredicted = 0\n",
    "    for index, (inputs, labels) in enumerate(dataloaders[set]):\n",
    "        \n",
    "        batch_size = len(inputs)\n",
    "        print(f\"At batch {index}/{dataset_sizes[set]}\")\n",
    "        with torch.no_grad():\n",
    "            alreadyPredicted = True\n",
    "            for i, seq in enumerate(inputs):\n",
    "                file = str(index*batch_size+i)+\".pt\"\n",
    "                filePath = os.path.join(setDir, file)\n",
    "                \n",
    "                if not os.path.exists(filePath):\n",
    "                    alreadyPredicted = False\n",
    "                    break\n",
    "            \n",
    "            if not alreadyPredicted:\n",
    "                print(f\"Predicting\")\n",
    "                esm_output = esmfold.infer(sequences=inputs)\n",
    "                s_s = esm_output[\"s_s\"]\n",
    "                batchesPredicted +=1\n",
    "                #s_z = esm_output[\"s_z\"]\n",
    "                with open(labelsFilePath,\"a\") as csv:\n",
    "                    for s, data in enumerate(s_s):\n",
    "                        file = str(index*batch_size+s)+\".pt\"\n",
    "\n",
    "                        if not os.path.exists(file):\n",
    "                            with open(os.path.join(setDir, file), \"wb\") as f:\n",
    "                                torch.save(data.cpu(),f)\n",
    "                            csv.write(f\"{file}, {labels[s][0].item()}, {inputs[s]}\\n\") \n",
    "        if index %5 == 0:\n",
    "            secsSpent = time.time()- timeStart  \n",
    "            secsToGo = (secsSpent/(batchesPredicted+1))*(dataset_sizes[set]/batch_size-index-1)\n",
    "            hoursToGo = secsToGo/(60*60)\n",
    "            now = datetime.datetime.now()\n",
    "            telegramBot.edit_text_message(messageId, f\"Done with {index}/{len(dataloaders[set])} batches for {set} (hours to go: {int(hoursToGo)}) [last update: {now.hour}:{now.minute}]\")\n",
    "        \n",
    "\n",
    "\n",
    "try:\n",
    "    for set in [\"train\", \"val\"]:\n",
    "        generateRepresenations(set)\n",
    "except Exception as e:\n",
    "    print(\"Exception raised: \", e)\n",
    "    telegramBot.send_telegram(\"Generation of representations failed with error message: \"+str(e))\n",
    "\n",
    "telegramBot.send_telegram(f\"Doneinger!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotprot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ce9c753ae11230af47747c2525775742a5c4219355d1b58136056d9f8dd6ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
