{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorganize representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename, thermostability, sequence\n",
    "import os\n",
    "seqToFile = {}\n",
    "\n",
    "def addToEntries(set):\n",
    "    with open(f\"data/s_s/{set}/labels.csv\", \"r\") as f:\n",
    "        firstLine = True\n",
    "\n",
    "        for line in f:\n",
    "            if firstLine:\n",
    "                firstLine = False\n",
    "                continue\n",
    "            filename, thermostability, sequence = line.split(\", \")\n",
    "            seqToFile[sequence] = f\"{set}/{filename}\"\n",
    "addToEntries(\"train\")\n",
    "addToEntries(\"val\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge val and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "with open(\"data/s_s/sequences.csv\", \"w\") as f:\n",
    "    f.write(\"sequence, filename\\n\")\n",
    "    for index, (seq, filename) in enumerate(seqToFile.items()):\n",
    "        sourcePath = os.path.join(\"data/s_s\", filename)\n",
    "        targetFilename = f\"{index}.pt\"\n",
    "        targetPath = f\"data/s_s/{targetFilename}\"\n",
    "\n",
    "        shutil.copyfile(sourcePath, targetPath)\n",
    "        f.write(f\"{seq}, {targetFilename}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train/eval metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_eval_ids = set()\n",
    "with open('data/uniref201803_ur50_valid_headers.txt') as txt_file:\n",
    "    for line in txt_file:\n",
    "        id = line.split('_')[1].replace('\\n','')\n",
    "        esm_eval_ids.add(id)\n",
    "\n",
    "def readFasta(filepath='data/full_dataset_sequences.fasta'):\n",
    "    first = True\n",
    "    max =0\n",
    "    dataset = []\n",
    "    with open(filepath) as fasta:\n",
    "        for line in fasta:\n",
    "            if line[0] == '>':\n",
    "                if first:\n",
    "                    first = False\n",
    "                else:\n",
    "                    dataset.append(entry)\n",
    "                entry = {}\n",
    "                header_tokens = line.split(' ')\n",
    "                entry['id'] = header_tokens[0].replace('>','').split('_')[0]\n",
    "                entry['header'] = line.replace('\\n', '')\n",
    "                entry['temp'] = float(header_tokens[1].split('=')[1].replace('\\n',''))\n",
    "                entry['sequence'] = ''\n",
    "            else:\n",
    "                entry['sequence'] = entry['sequence'] + line.replace('\\n','')\n",
    "                max = len(entry['sequence']) if len(entry['sequence'])> max else max\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "evalDs = []\n",
    "trainUnfilteredDs = []\n",
    "trainIds = set()\n",
    "allIds = set()\n",
    "dataset = readFasta()\n",
    "for entry in dataset:\n",
    "    seq = entry['sequence']\n",
    "    id = entry[\"id\"]\n",
    "    allIds.add(id)\n",
    "    if id in esm_eval_ids:\n",
    "        evalDs.append(entry)\n",
    "    else: \n",
    "        trainUnfilteredDs.append(entry)\n",
    "        trainIds.add(id)\n",
    "\n",
    "# Filter train ds\n",
    "clusters = {}\n",
    "with open(\"data/meltome_PIDE20_clusters.tsv\", \"r\") as f:\n",
    "    firstLine = True\n",
    "    for line in f:\n",
    "        if firstLine:\n",
    "          firstLine = False\n",
    "          continue   \n",
    "        clusterId, proteinId = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        proteinId = proteinId.split('_')[0]\n",
    "        \n",
    "        if proteinId in allIds:\n",
    "            if clusterId in clusters:\n",
    "                clusters[clusterId].add(proteinId)\n",
    "            else: \n",
    "                clusters[clusterId] = set([proteinId])\n",
    "\n",
    "\n",
    "for clusterId, proteinids in clusters.items():\n",
    "  numTrain = 0\n",
    "  numEval = 0\n",
    "  for proteinId in proteinids:\n",
    "    if proteinId in esm_eval_ids:\n",
    "      numEval += 1\n",
    "    else: \n",
    "      numTrain += 1\n",
    "    \n",
    "    if numEval > 0 and numTrain>0:\n",
    "        for proteinId in proteinids:\n",
    "            if proteinId in trainIds:\n",
    "                trainIds.remove(proteinId)\n",
    "\n",
    "trainDs = [item for item in trainUnfilteredDs if item[\"id\"] in trainIds]\n",
    "\n",
    "\n",
    "def storeMetadata(ds,set: str):\n",
    "    with open(f\"data/s_s/{set}.csv\", \"w\") as f:\n",
    "        f.write(\"sequence, melting point\\n\")\n",
    "        for entry in ds:\n",
    "            f.write(f'{entry[\"sequence\"]}, {entry[\"temp\"]}\\n')\n",
    "\n",
    "storeMetadata(trainDs, \"train\")\n",
    "storeMetadata(evalDs, \"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotprot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ce9c753ae11230af47747c2525775742a5c4219355d1b58136056d9f8dd6ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
