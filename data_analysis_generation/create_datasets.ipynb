{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-/Validation split creation\n",
    "In this notebook we go through our process of creating the train dataset as well as the validation dataset. \n",
    "In the context of this project a dataset means a set of `(protein sequence, thermostability)` pairs. \n",
    "For a given protein sequence there may be multiple thermostability value measurements as given by the FLIP dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read FLIP dataset\n",
    "The source of the dataset file `full_dataset_sequences.fasta` is [here](https://github.com/J-SNACKKB/FLIP/tree/main/splits/meltome). Among other information it contains protein sequences and corresponding thermostability (melting point) measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(filepath=\"../data/full_dataset_sequences.fasta\"):\n",
    "    first = True\n",
    "    max = 0\n",
    "    dataset = []\n",
    "    with open(filepath) as fasta:\n",
    "        for line in fasta:\n",
    "            if line[0] == \">\":\n",
    "                if first:\n",
    "                    first = False\n",
    "                else:\n",
    "                    dataset.append(entry)\n",
    "                entry = {}\n",
    "                header_tokens = line.split(\" \")\n",
    "                entry[\"id\"] = header_tokens[0].replace(\">\", \"\").split(\"_\")[0]\n",
    "                entry[\"header\"] = line.replace(\"\\n\", \"\")\n",
    "                entry[\"temp\"] = float(header_tokens[1].split(\"=\")[1].replace(\"\\n\", \"\"))\n",
    "                entry[\"sequence\"] = \"\"\n",
    "            else:\n",
    "                entry[\"sequence\"] = entry[\"sequence\"] + line.replace(\"\\n\", \"\")\n",
    "                max = len(entry[\"sequence\"]) if len(entry[\"sequence\"]) > max else max\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "flip_dataset = read_fasta()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read ESM validation protein clusters\n",
    "ESM validation protein clusters are those Uniref100 clusters that were held out during training of ESM2 and ESMFold. \n",
    "As we are basically doing transfer learning on top of the ESMFold outputs, we also use these as a validation/test set, avoiding any potential data leakage due to ESMFold having seen proteins during training that we are using during validation/testing. \n",
    "More info and a download link can be found [here](https://github.com/facebookresearch/esm#pre-training-dataset-split--) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_eval_clusters = dict()\n",
    "esm_eval_ids = set()\n",
    "with open('../data/uniref201803_ur100_valid_headers.txt') as txt_file:\n",
    "    for line in txt_file:\n",
    "        parts = line.split(\" \")\n",
    "        id = parts[0].split('_')[1]\n",
    "        cluster = parts[1].split(\"_\")[1].replace('\\n','')\n",
    "        esm_eval_ids.add(id)\n",
    "        if cluster not in esm_eval_clusters:\n",
    "            esm_eval_clusters[cluster] = []\n",
    "        esm_eval_clusters[cluster].append(id)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create held out dataset (test/val) and train ids set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "held_out_dataset = []\n",
    "train_dataset = []\n",
    "train_ids = set()\n",
    "all_ids = set()\n",
    "dataset = read_fasta()\n",
    "for entry in dataset:\n",
    "    seq = entry[\"sequence\"]\n",
    "    id = entry[\"id\"]\n",
    "    all_ids.add(id)\n",
    "    if id in esm_eval_ids:\n",
    "        held_out_dataset.append(entry)\n",
    "    else: \n",
    "        train_dataset.append(entry)\n",
    "        train_ids.add(id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split held out dataset by cluster\n",
    "To avoid optimizing the hyperparameters of our model for the given validation set, we also introduce a test set. The test set is then only used for final evaluation of the model. However hyperparameters won't be optimized for best performance on the test set but on the validation set. \n",
    "\n",
    "To avoid any similar protein sequences being present in our test and validation set, we construct these sets by randomly splitting the held out set (i.e. the non-training set) by their UniRef100 cluster. While doing this each cluster has a 2/3rd chance to be added to the validation set and a 1/3rd chance of being added to the test set respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = set()\n",
    "val_ids = set()\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "for cluster_id, protein_ids in esm_eval_clusters.items():\n",
    "    is_test = random.random() <= 1/3\n",
    "    for protein_id in protein_ids:\n",
    "        (test_ids if is_test else val_ids).add(protein_id)\n",
    "    \n",
    "test_dataset = [item for item in held_out_dataset if item[\"id\"] in test_ids ]\n",
    "val_dataset = [item for item in held_out_dataset if item[\"id\"] in val_ids ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store val.csv and train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Info for train set-----\n",
      "len 181136\n",
      "Num unique sequences:  30988\n",
      "Num unique sequences (len < 700):  24411\n",
      "-----Info for val set-----\n",
      "len 12760\n",
      "Num unique sequences:  2129\n",
      "Num unique sequences (len < 700):  1672\n",
      "-----Info for test set-----\n",
      "len 7386\n",
      "Num unique sequences:  1091\n",
      "Num unique sequences (len < 700):  861\n"
     ]
    }
   ],
   "source": [
    "def storeMetadata(ds, name: str):\n",
    "    with open(f\"../data/{name}.csv\", \"w\") as f:\n",
    "        f.write(\"sequence, melting point\\n\")\n",
    "        for entry in ds:\n",
    "            f.write(f'{entry[\"sequence\"]}, {entry[\"temp\"]}\\n')\n",
    "\n",
    "\n",
    "def print_ds_infos(name: str, ds: list):\n",
    "    unique_seqs = set([entry[\"sequence\"] for entry in ds])\n",
    "    print(f'{\"-\"*5}Info for {name} set{\"-\"*5}')\n",
    "    print(\"len\", len(ds))\n",
    "    print(\"Num unique sequences: \", len(unique_seqs))\n",
    "    print(\n",
    "        \"Num unique sequences (len < 700): \",\n",
    "        len([seq for seq in unique_seqs if len(seq) < 700]),\n",
    "    )\n",
    "\n",
    "print_ds_infos(\"train\", train_dataset)\n",
    "print_ds_infos(\"val\", val_dataset)\n",
    "print_ds_infos(\"test\", test_dataset)\n",
    "\n",
    "storeMetadata(train_dataset, \"train\")\n",
    "storeMetadata(val_dataset, \"val\")\n",
    "storeMetadata(test_dataset, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
