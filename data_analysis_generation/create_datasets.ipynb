{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-/Validation split creation\n",
    "In this notebook we go through our process of creating the train dataset as well as the validation dataset. \n",
    "In the context of this project a dataset means a set of `(protein sequence, thermostability)` pairs. \n",
    "For a given protein sequence there may be multiple thermostability value measurements as given by the FLIP dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read FLIP dataset\n",
    "The source of the dataset file `full_dataset_sequences.fasta` is [here](https://github.com/J-SNACKKB/FLIP/tree/main/splits/meltome). Among other information it contains protein sequences and corresponding thermostability (melting point) measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(filepath='../data/full_dataset_sequences.fasta'):\n",
    "    first = True\n",
    "    max =0\n",
    "    dataset = []\n",
    "    with open(filepath) as fasta:\n",
    "        for line in fasta:\n",
    "            if line[0] == '>':\n",
    "                if first:\n",
    "                    first = False\n",
    "                else:\n",
    "                    dataset.append(entry)\n",
    "                entry = {}\n",
    "                header_tokens = line.split(' ')\n",
    "                entry['id'] = header_tokens[0].replace('>','').split('_')[0]\n",
    "                entry['header'] = line.replace('\\n', '')\n",
    "                entry['temp'] = float(header_tokens[1].split('=')[1].replace('\\n',''))\n",
    "                entry['sequence'] = ''\n",
    "            else:\n",
    "                entry['sequence'] = entry['sequence'] + line.replace('\\n','')\n",
    "                max = len(entry['sequence']) if len(entry['sequence'])> max else max\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "flip_dataset = read_fasta()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read ESM validation protein ids\n",
    "ESM validation protein ids are those Uniref50 cluster representative protein ids that were held out during training of ESM2 and ESMFold. \n",
    "As we are basically doing transfer learning on top of the ESMFold outputs, we also use these as a validation set, avoiding any potential data leakage due to ESMFold having seen proteins during training that we are using during validation. \n",
    "More info and a download link can be found [here](https://github.com/facebookresearch/esm#pre-training-dataset-split--) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_eval_ids = set()\n",
    "with open('../data/uniref201803_ur50_valid_headers.txt') as txt_file:\n",
    "    for line in txt_file:\n",
    "        id = line.split('_')[1].replace('\\n','')\n",
    "        esm_eval_ids.add(id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation dataset and unfiltered train ids set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = []\n",
    "train_unfiltered_dataset = []\n",
    "train_ids = set()\n",
    "all_ids = set()\n",
    "dataset = read_fasta()\n",
    "for entry in dataset:\n",
    "    seq = entry['sequence']\n",
    "    id = entry[\"id\"]\n",
    "    all_ids.add(id)\n",
    "    if id in esm_eval_ids:\n",
    "        eval_dataset.append(entry)\n",
    "    else: \n",
    "        train_unfiltered_dataset.append(entry)\n",
    "        train_ids.add(id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter train dataset by cluster\n",
    "To avoid any similar protein sequences being present in our train and validation, we filter our train set to only contain proteins that are not part of the same cluster as proteins in our validation set, based on the [FLIP clustering](https://github.com/J-SNACKKB/FLIP/blob/main/splits/meltome/splits.zip)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read clusters and associated protein ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "with open(\"../data/meltome_PIDE20_clusters.tsv\", \"r\") as f:\n",
    "    firstLine = True\n",
    "    for line in f:\n",
    "        if firstLine:\n",
    "          firstLine = False\n",
    "          continue   \n",
    "        cluster_id, protein_id = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        protein_id = protein_id.split('_')[0]\n",
    "        \n",
    "        if protein_id in all_ids:\n",
    "            if cluster_id in clusters:\n",
    "                clusters[cluster_id].add(protein_id)\n",
    "            else: \n",
    "                clusters[cluster_id] = set([protein_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id, protein_ids in clusters.items():\n",
    "    num_train = 0\n",
    "    num_eval = 0\n",
    "    for protein_id in protein_ids:\n",
    "        if protein_id in esm_eval_ids:\n",
    "            num_eval += 1\n",
    "        else: \n",
    "            num_train += 1\n",
    "    \n",
    "    if num_eval > 0 and num_train>0:\n",
    "        for protein_id in protein_ids:\n",
    "            if protein_id in train_ids:\n",
    "                train_ids.remove(protein_id)\n",
    "\n",
    "train_dataset = [item for item in train_unfiltered_dataset if item[\"id\"] in train_ids]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store val.csv and train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeMetadata(ds,name: str):\n",
    "    with open(f\"../data/s_s/{name}.csv\", \"w\") as f:\n",
    "        f.write(\"sequence, melting point\\n\")\n",
    "        for entry in ds:\n",
    "            f.write(f'{entry[\"sequence\"]}, {entry[\"temp\"]}\\n')\n",
    "\n",
    "storeMetadata(train_dataset, \"train\")\n",
    "storeMetadata(eval_dataset, \"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
