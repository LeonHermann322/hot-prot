{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorganize representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename, thermostability, sequence\n",
    "import os\n",
    "seqToFile = {}\n",
    "\n",
    "def addToEntries(set):\n",
    "    with open(f\"data/s_s/{set}/labels.csv\", \"r\") as f:\n",
    "        firstLine = True\n",
    "\n",
    "        for line in f:\n",
    "            if firstLine:\n",
    "                firstLine = False\n",
    "                continue\n",
    "            filename, thermostability, sequence = line.split(\", \")\n",
    "            sequence = sequence.replace(\"\\n\", \"\")\n",
    "            seqToFile[sequence] = f\"{set}/{filename}\"\n",
    "addToEntries(\"train\")\n",
    "addToEntries(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNYKNDTMIRKEATVSESVLSELKRIIEDSEIMQEDDDNWPEPDKIGRQELEILYKNEHISFTTGKIGALADVNNSKDPDGLRSFYYLVQDLKCLVFSLIGLHFKIKPI',\n",
       " 'train/0.pt')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(seqToFile.items()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge val and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "with open(\"data/s_s/sequences.csv\", \"w\") as f:\n",
    "    f.write(\"sequence, filename\\n\")\n",
    "    for index, (seq, filename) in enumerate(seqToFile.items()):\n",
    "        sourcePath = os.path.join(\"data/s_s\", filename)\n",
    "        targetFilename = f\"{index}.pt\"\n",
    "        targetPath = f\"data/s_s/{targetFilename}\"\n",
    "\n",
    "        shutil.copyfile(sourcePath, targetPath)\n",
    "        f.write(f\"{seq}, {targetFilename}\\n\")\n",
    "        print(f\"Done with {index}/{len(seqToFile.items())}\", end=\"\\r\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train/eval metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_eval_ids = set()\n",
    "with open('data/uniref201803_ur50_valid_headers.txt') as txt_file:\n",
    "    for line in txt_file:\n",
    "        id = line.split('_')[1].replace('\\n','')\n",
    "        esm_eval_ids.add(id)\n",
    "\n",
    "def readFasta(filepath='data/full_dataset_sequences.fasta'):\n",
    "    first = True\n",
    "    max =0\n",
    "    dataset = []\n",
    "    with open(filepath) as fasta:\n",
    "        for line in fasta:\n",
    "            if line[0] == '>':\n",
    "                if first:\n",
    "                    first = False\n",
    "                else:\n",
    "                    dataset.append(entry)\n",
    "                entry = {}\n",
    "                header_tokens = line.split(' ')\n",
    "                entry['id'] = header_tokens[0].replace('>','').split('_')[0]\n",
    "                entry['header'] = line.replace('\\n', '')\n",
    "                entry['temp'] = float(header_tokens[1].split('=')[1].replace('\\n',''))\n",
    "                entry['sequence'] = ''\n",
    "            else:\n",
    "                entry['sequence'] = entry['sequence'] + line.replace('\\n','')\n",
    "                max = len(entry['sequence']) if len(entry['sequence'])> max else max\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "evalDs = []\n",
    "trainUnfilteredDs = []\n",
    "trainIds = set()\n",
    "allIds = set()\n",
    "dataset = readFasta()\n",
    "for entry in dataset:\n",
    "    seq = entry['sequence']\n",
    "    id = entry[\"id\"]\n",
    "    allIds.add(id)\n",
    "    if id in esm_eval_ids:\n",
    "        evalDs.append(entry)\n",
    "    else: \n",
    "        trainUnfilteredDs.append(entry)\n",
    "        trainIds.add(id)\n",
    "\n",
    "# Filter train ds\n",
    "clusters = {}\n",
    "with open(\"data/meltome_PIDE20_clusters.tsv\", \"r\") as f:\n",
    "    firstLine = True\n",
    "    for line in f:\n",
    "        if firstLine:\n",
    "          firstLine = False\n",
    "          continue   \n",
    "        clusterId, proteinId = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        proteinId = proteinId.split('_')[0]\n",
    "        \n",
    "        if proteinId in allIds:\n",
    "            if clusterId in clusters:\n",
    "                clusters[clusterId].add(proteinId)\n",
    "            else: \n",
    "                clusters[clusterId] = set([proteinId])\n",
    "\n",
    "\n",
    "for clusterId, proteinids in clusters.items():\n",
    "  numTrain = 0\n",
    "  numEval = 0\n",
    "  for proteinId in proteinids:\n",
    "    if proteinId in esm_eval_ids:\n",
    "      numEval += 1\n",
    "    else: \n",
    "      numTrain += 1\n",
    "    \n",
    "    if numEval > 0 and numTrain>0:\n",
    "        for proteinId in proteinids:\n",
    "            if proteinId in trainIds:\n",
    "                trainIds.remove(proteinId)\n",
    "\n",
    "trainDs = [item for item in trainUnfilteredDs if item[\"id\"] in trainIds]\n",
    "\n",
    "\n",
    "def storeMetadata(ds,set: str):\n",
    "    with open(f\"data/{set}.csv\", \"w\") as f:\n",
    "        f.write(\"sequence, melting point\\n\")\n",
    "        for entry in ds:\n",
    "            f.write(f'{entry[\"sequence\"]}, {entry[\"temp\"]}\\n')\n",
    "\n",
    "storeMetadata(trainDs, \"train\")\n",
    "storeMetadata(evalDs, \"val\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate missing representations\n",
    "## Find missing sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sequence sample MIKLFSLKQQKKDEESAGGPRAGGGGKKASAAQLRIQKDINELNLPKTCEIVFPDQDDLLNFKLIISPDEGFYKGGKFVFSFKGRVIRTTPPKSSVKRWFITPTLTWRETSV\n",
      "Generated sequence sample MTPSTPPRSRGTRYLAQPSGNTSSSALMQGQKTPQKPSQNLVPVTPSTTKSFKNAPLLAPPNSNMGMTSPFNGLTSPQRSPFPKSSVKRTLFQFESHDNGTVREEQEPLGRVNRILFPTQQNVDIDAAEEEEEGEVLLPPSRPTSARQLHLSLERDEFDQTHRKKIIKDVPGTPSDKVITFELAKNWNNNSPKNDARSQESEDEEDIIINPVRVGKNPFASDELVTQEIRNERKRAMLRENPDIEDVITYVNKKGEVVEKRRLTDEEKRRFKPKALFQSRDQEH\n",
      "Required sequences 21713\n",
      "Generated sequences 16674\n",
      "Remaining sequences to be generated:  8277\n",
      "Generated but not required sequences:  3238\n"
     ]
    }
   ],
   "source": [
    "maxSeqLen = 700\n",
    "\n",
    "requiredSequences = set()\n",
    "def addSequencesFromMetadataFile(set):\n",
    "    with open(f\"data/{set}.csv\", \"r\") as f:\n",
    "        firstLine = True\n",
    "        for line in f:\n",
    "            if firstLine:\n",
    "                firstLine = False\n",
    "                continue   \n",
    "            sequence, _ = line.replace(\"\\n\", \"\").split(\", \")\n",
    "            if len(sequence) <=maxSeqLen:\n",
    "                requiredSequences.add(sequence)\n",
    "\n",
    "\n",
    "addSequencesFromMetadataFile(\"val\")\n",
    "addSequencesFromMetadataFile(\"train\")\n",
    "\n",
    "generatedSequences = set()\n",
    "\n",
    "with open(f\"data/s_s/sequences.csv\", \"r\") as f:\n",
    "    firstLine = True\n",
    "    for line in f:\n",
    "        if firstLine:\n",
    "            firstLine = False\n",
    "            continue   \n",
    "        sequence, _ = line.replace(\"\\n\", \"\").split(\", \")\n",
    "        \n",
    "        generatedSequences.add(sequence)\n",
    "\n",
    "print(\"Required sequence sample\", next(iter(requiredSequences)))\n",
    "print(\"Generated sequence sample\", next(iter(generatedSequences)))\n",
    "print(\"Required sequences\", len(requiredSequences))\n",
    "print(\"Generated sequences\", len(generatedSequences))\n",
    "print(\"Remaining sequences to be generated: \", len(requiredSequences.difference(generatedSequences)))\n",
    "print(\"Generated but not required sequences: \", len(generatedSequences.difference(requiredSequences)))\n",
    "\n",
    "\n",
    "remainingSequences = requiredSequences.difference(generatedSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(seq) for seq in remainingSequences])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate missing sequences\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from thermostability.thermo_dataset import ThermostabilityDataset\n",
    "from util.telegram import TelegramBot\n",
    "import esm\n",
    "import pickle\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.list_gpu_processes()\n",
    "\n",
    "telegramBot = TelegramBot()\n",
    "telegramBot.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMFold(\n",
       "  (esm): ESM2(\n",
       "    (embed_tokens): Embedding(33, 2560, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (12): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (13): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (14): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (15): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (16): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (17): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (18): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (19): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (20): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (21): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (22): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (23): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (24): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (25): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (26): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (27): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (28): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (29): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (30): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (31): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (32): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (33): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (34): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (35): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (contact_head): ContactPredictionHead(\n",
       "      (regression): Linear(in_features=1440, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "      (layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (esm_s_mlp): Sequential(\n",
       "    (0): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=2560, out_features=1024, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(23, 1024, padding_idx=0)\n",
       "  (trunk): FoldingTrunk(\n",
       "    (pairwise_positional_embedding): RelativePosition(\n",
       "      (embedding): Embedding(66, 128)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (28): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (29): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (30): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (31): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (32): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (33): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (34): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (35): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (36): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (37): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (38): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (39): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (40): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (41): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (42): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (43): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (44): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (45): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (46): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (47): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (recycle_s_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (recycle_z_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (recycle_disto): Embedding(15, 128)\n",
       "    (structure_module): StructureModule(\n",
       "      (layer_norm_s): LayerNorm()\n",
       "      (layer_norm_z): LayerNorm()\n",
       "      (linear_in): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (ipa): InvariantPointAttention(\n",
       "        (linear_q): Linear(in_features=384, out_features=192, bias=True)\n",
       "        (linear_kv): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (linear_q_points): Linear(in_features=384, out_features=144, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=384, out_features=432, bias=True)\n",
       "        (linear_b): Linear(in_features=128, out_features=12, bias=True)\n",
       "        (linear_out): Linear(in_features=2112, out_features=384, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (ipa_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm_ipa): LayerNorm()\n",
       "      (transition): StructureModuleTransition(\n",
       "        (layers): ModuleList(\n",
       "          (0): StructureModuleTransitionLayer(\n",
       "            (linear_1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (linear_2): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (linear_3): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm()\n",
       "      )\n",
       "      (bb_update): BackboneUpdate(\n",
       "        (linear): Linear(in_features=384, out_features=6, bias=True)\n",
       "      )\n",
       "      (angle_resnet): AngleResnet(\n",
       "        (linear_in): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (linear_initial): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): AngleResnetBlock(\n",
       "            (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): AngleResnetBlock(\n",
       "            (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (linear_out): Linear(in_features=128, out_features=14, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (trunk2sm_s): Linear(in_features=1024, out_features=384, bias=True)\n",
       "    (trunk2sm_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (distogram_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (ptm_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lm_head): Linear(in_features=1024, out_features=23, bias=True)\n",
       "  (lddt_head): Sequential(\n",
       "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=1850, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esmfold = esm.pretrained.esmfold_v1()\n",
    "esmfold.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define sequences DS for convenience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SequencesDataset(Dataset):\n",
    "    def __init__(self, sequences: \"set[str]\") -> None:\n",
    "        super().__init__()\n",
    "        self.sequences = list(sequences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with maxFilePrefix 16673\n",
      "At batch 0/4138\n",
      "Predicting\n",
      "At batch 1/4138\n",
      "Predicting\n",
      "At batch 2/4138\n",
      "Predicting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_788500/1362100133.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mgenerateRepresenations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremainingSequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exception raised: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_788500/1362100133.py\u001b[0m in \u001b[0;36mgenerateRepresenations\u001b[0;34m(sequences)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0malreadyPredicted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predicting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mesm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesmfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0ms_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s_s\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mbatchesPredicted\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/esmfold.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, sequences, residx, masking_pattern, num_recycles, residue_index_offset, chain_linker)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mresidx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mmasking_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasking_pattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mnum_recycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_recycles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/esmfold.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, aa, mask, residx, masking_pattern, num_recycles)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         structure: dict = self.trunk(\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0ms_s_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_recycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_recycles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         )\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Documenting what we expect:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/trunk.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles)\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mrecycle_z\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecycle_disto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecycle_bins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0ms_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrunk_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_s_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecycle_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_z_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecycle_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;31m# === Structure module ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/trunk.py\u001b[0m in \u001b[0;36mtrunk_iter\u001b[0;34m(s, z, residx, mask)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidue_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/esm/esmfold/v1/tri_self_attn_block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequence_state, pairwise_state, mask, chunk_size, **_TriangularSelfAttentionBlock__kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m         pairwise_state = pairwise_state + self.col_drop(\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtri_mul_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairwise_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtri_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         )\n\u001b[1;32m    150\u001b[0m         pairwise_state = pairwise_state + self.row_drop(\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/openfold/model/triangular_multiplicative_update.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, mask, inplace_safe, _add_with_inplace, _inplace_chunk_size)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/envs/hotprot/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "rootDir = \"data/s_s/\"\n",
    "\n",
    "mergedDsSeqToFile = {}\n",
    "labelsFilePath = \"data/s_s/sequences.csv\"\n",
    "with open(labelsFilePath, \"r\") as f:\n",
    "    firstLine = True\n",
    "    for line in f:\n",
    "        if firstLine:\n",
    "            firstLine = False\n",
    "            continue\n",
    "        sequence, filename = line.replace(\"\\n\", \"\").split(\", \")\n",
    "        mergedDsSeqToFile[sequence] = filename\n",
    "\n",
    "def generateRepresenations(sequences):\n",
    "    ds = SequencesDataset(sequences)\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=False, num_workers=0)\n",
    "    timeStart = time.time()\n",
    "    telegramBot.send_telegram(f\"Generating remaining s_s representations for {len(sequences)} sequences\")\n",
    "    response = telegramBot.send_telegram(f\"Generating first batch...\")\n",
    "    messageId = response[\"result\"][\"message_id\"]\n",
    "\n",
    "    \n",
    "    if not os.path.exists(labelsFilePath):\n",
    "        with open(labelsFilePath,\"w\") as csv:\n",
    "            csv.write(f\"sequence, filename\\n\") \n",
    "\n",
    "    maxFilePrefix = 0\n",
    "    for seq, file in mergedDsSeqToFile.items():\n",
    "        prefix = int(file.split(\".\")[0])\n",
    "        if prefix > maxFilePrefix:\n",
    "            maxFilePrefix = prefix\n",
    "\n",
    "    print(f\"Starting with maxFilePrefix {maxFilePrefix}\")\n",
    "    batchesPredicted = 0\n",
    "\n",
    "    for index, (inputs) in enumerate(loader):\n",
    "        \n",
    "        batch_size = len(inputs)\n",
    "        numBatches = int(len(sequences) / batch_size)\n",
    "        print(f\"At batch {index}/{numBatches}\")\n",
    "        with torch.no_grad():\n",
    "            alreadyPredicted = True\n",
    "            for i, seq in enumerate(inputs):\n",
    "                if seq not in mergedDsSeqToFile:\n",
    "                    alreadyPredicted = False\n",
    "                    break\n",
    "            \n",
    "            if not alreadyPredicted:\n",
    "                print(f\"Predicting\")\n",
    "                esm_output = esmfold.infer(sequences=inputs)\n",
    "                s_s = esm_output[\"s_s\"]\n",
    "                batchesPredicted +=1\n",
    "                #s_z = esm_output[\"s_z\"]\n",
    "                with open(labelsFilePath,\"a\") as csv:\n",
    "                    for s, data in enumerate(s_s):\n",
    "                        maxFilePrefix+=1\n",
    "                        file = str(maxFilePrefix)+\".pt\"\n",
    "\n",
    "                        if not os.path.exists(file):\n",
    "                            with open(os.path.join(\"data/s_s\", file), \"wb\") as f:\n",
    "                                torch.save(data.cpu(),f)\n",
    "                            csv.write(f\"{inputs[s]}, {file}\\n\") \n",
    "        if index %5 == 0:\n",
    "            secsSpent = time.time()- timeStart  \n",
    "            secsToGo = (secsSpent/(batchesPredicted+1))*(numBatches-index-1)\n",
    "            hoursToGo = secsToGo/(60*60)\n",
    "            now = datetime.datetime.now()\n",
    "            telegramBot.edit_text_message(messageId, f\"Done with {index}/{numBatches} batches (hours to go: {int(hoursToGo)}) [last update: {now.hour}:{now.minute}]\")\n",
    "        \n",
    "\n",
    "\n",
    "try:\n",
    "    generateRepresenations(remainingSequences)\n",
    "except Exception as e:\n",
    "    print(\"Exception raised: \", e)\n",
    "    telegramBot.send_telegram(\"Generation of representations failed with error message: \"+str(e))\n",
    "\n",
    "telegramBot.send_telegram(f\"Doneinger!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import torch\n",
    "repr_root = \"../data/s_s\"\n",
    "\n",
    "with open(os.path.join(repr_root,\"sequences.csv\"), \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=',', skipinitialspace=True)\n",
    "    seqToFile = dict([(seq, os.path.join(repr_root,file_name)) for i, (seq, file_name) in enumerate(reader) if i!=0])\n",
    "\n",
    "avg_repr_root = \"../data/s_s_avg\"\n",
    "if not os.path.exists(avg_repr_root):\n",
    "    os.mkdir(avg_repr_root)\n",
    "with open(os.path.join(avg_repr_root,\"sequences.csv\"), \"w\") as f:\n",
    "    f.write(\"sequence, filename\\n\")\n",
    "    for (seq, repr_file_path) in seqToFile.items():\n",
    "        s_s = torch.load(repr_file_path)\n",
    "        avg_repr_f_name = os.path.basename(repr_file_path)\n",
    "        torch.save(torch.mean(s_s, 0), os.path.join(avg_repr_root,avg_repr_f_name))\n",
    "        f.write(f\"{seq}, {avg_repr_f_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotprot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ce9c753ae11230af47747c2525775742a5c4219355d1b58136056d9f8dd6ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
